<!DOCTYPE html>
<html lang="en"><head><title>Bots to assist in reading</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="🪴 Quartz 4.0"/><meta property="og:title" content="Bots to assist in reading"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Bots to assist in reading"/><meta name="twitter:description" content="Organize the ideas you have written down in Thinking about speed reading. We need clarity of purpose. The goal of eliminating piles of reading Newly arrived books can now be looked through on the same day."/><meta property="og:description" content="Organize the ideas you have written down in Thinking about speed reading. We need clarity of purpose. The goal of eliminating piles of reading Newly arrived books can now be looked through on the same day."/><meta property="og:image:type" content="image/webp"/><meta property="og:image:alt" content="Organize the ideas you have written down in Thinking about speed reading. We need clarity of purpose. The goal of eliminating piles of reading Newly arrived books can now be looked through on the same day."/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:width" content="1200"/><meta property="og:height" content="630"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https:/quartz.jzhao.xyz/Bots-to-assist-in-reading"/><meta property="twitter:url" content="https:/quartz.jzhao.xyz/Bots-to-assist-in-reading"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="Organize the ideas you have written down in Thinking about speed reading. We need clarity of purpose. The goal of eliminating piles of reading Newly arrived books can now be looked through on the same day."/><meta name="generator" content="Quartz"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("./static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Bots-to-assist-in-reading"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href=".">🪴 Quartz 4.0</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="./">Home</a><p> ❯ </p></div><div class="breadcrumb-element"><a href>Bots to assist in reading</a></div></nav><h1 class="article-title">Bots to assist in reading</h1><p show-comma="true" class="content-meta"><span>Oct 27, 2021</span><span>10 min read</span></p></div></div><article class="popover-hint"><ul>
<li>
<p>Organize the ideas you have written down in <a href="./Thinking-about-speed-reading" class="internal alias" data-slug="Thinking-about-speed-reading">Thinking about speed reading</a>.</p>
</li>
<li>
<p>We need clarity of purpose.</p>
<ul>
<li>The goal of eliminating piles of reading
<ul>
<li>Newly arrived books can now be looked through on the same day.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>The following achievable objectives need to be identified</p>
</li>
<li>
<p>It’s not a good idea to leave a “I meant to read it, but I didn’t.”</p>
<ul>
<li>If you decide to read it, you should.</li>
<li>If you don’t read it and keep it.
<ul>
<li>If you expect to get a search hit in the future when you need it.</li>
<li>You should make sure you get a hit in your search.</li>
</ul>
</li>
<li>If it’s neither, it should be discarded.</li>
<li>Should be “handled” one way or the other</li>
</ul>
</li>
<li>
<p>We should “look over” everything at a level high enough to determine that.</p>
<ul>
<li>Some of the things that are being “looked over”, including books, should be read more carefully</li>
</ul>
</li>
<li>
<p>If you download a paper you intend to read, you should look over it that day.</p>
</li>
<li>
<p>When you receive a subscription to an academic journal or other publication, you should look through it that day.</p>
<ul>
<li><a href="./Should→why" class="internal alias" data-slug="Should→why">Should→why?</a></li>
</ul>
</li>
<li>
<p>What was not worth reading today will be worth reading tomorrow?</p>
<ul>
<li>Can’t say I wouldn’t get it…</li>
</ul>
</li>
<li>
<p>Should I read what I have already accumulated?</p>
</li>
<li>
<p>What does “reading carefully” mean?</p>
<ul>
<li>To “read well” means to invest more time in reading the book</li>
<li>Investments are made with the indication that more new ones are likely to be obtained.</li>
<li>What’s the point of reading a book to gain new insights?
<ul>
<li>Trying to answer this question in general would be too abstract.
<ul>
<li>= Contrary to the fact that goals must be specific</li>
</ul>
</li>
<li>General answer
<ul>
<li>
<p>To improve the model in one’s brain and increase the probability of being able to cope with possible unknowns in the future.</p>
</li>
<li>
<p>To improve my model, I need to encounter things that cannot be explained by my current model or that contradict the explanation in my model.</p>
</li>
<li>
<p>It’s what you can’t easily swallow that’s worth chewing on.</p>
<ul>
<li><a href="./Don't-aim-to-encounter-the-unknown." class="internal alias" data-slug="Don't-aim-to-encounter-the-unknown.">Don’t aim to encounter the unknown.</a></li>
<li>It reduces motivation because the effort is not reliably accomplished.
<ul>
<li>Really?
<ul>
<li>When it comes to work that is assigned to you, if you don’t get results despite your efforts, you feel demotivated.</li>
<li>But from a <a href="./flow-theory" class="internal alias" data-slug="flow-theory">flow theory</a> point of view, a success rate of 1/3 is the most exciting.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Goals for a quick sense of accomplishment are different from goals for a sense of accomplishment</p>
</li>
<li>
<p>Probabilities cannot be known in advance, so it’s difficult to adjust them yourself.</p>
</li>
<li>
<p>Is it easier to get into the flow if you can adjust the difficulty of the task yourself?</p>
<ul>
<li>A near and surely achievable goal is to keep track of the number of books you read and the amount of time you spend reading them.
<ul>
<li>Now they’re being recorded individually.
<ul>
<li>How many books have you read in total?</li>
<li>And how long did that take?</li>
<li>How about visualization?</li>
</ul>
</li>
</ul>
</li>
<li>The goal, which is a distant and challenging one, is to find something interesting.
<ul>
<li>Related: <a href="./Stretch-Goal-Setting" class="internal alias" data-slug="Stretch-Goal-Setting">Stretch Goal Setting</a>.</li>
<li>n or more per week.
<ul>
<li>With this way of setting up, you can adjust by increasing the amount of reading or daring to try different fields when you cannot find them.</li>
<li>Record the time taken.</li>
<li>You can measure the difficulty and adjust n to achieve a reasonable balance.</li>
</ul>
</li>
<li>You can blog about interesting things you find.
<ul>
<li>Reinforcement of preferred behavior by linking it to the need for approval.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>First, a tally of what we’ve done so far, then measurements for the future.</p>
</li>
<li>
<p>Attending study groups, like reading, is also a mess where you invest time to get something interesting.</p>
</li>
<li>
<p>Let’s record everything.</p>
<ul>
<li>Let’s tally up, week by week, the time and money invested and the interesting things gained.</li>
</ul>
</li>
<li>
<p>Do not envision a system that is too complex from the start.</p>
<ul>
<li>I hate it all when I fail to execute due to my poor administrative skills.</li>
<li>First, it should be implemented minimally to validate value.</li>
</ul>
</li>
<li>
<p>I would like to support this with a bot.</p>
<ul>
<li>Recommendation from sentence to sentence</li>
<li>Suggestions with sources in blog corpus and library corpus</li>
<li>Enhanced with a like.</li>
</ul>
</li>
<li>
<p>I’m thinking of the hippocampus model.</p>
</li>
<li>
<p>Image of a rat running around</p>
<ul>
<li>Running or turning at zero knowledge is randomly determined.</li>
<li>Take actions that seem good when you have knowledge, but also explore, i.e., <a href="./reinforcement-learning" class="internal alias" data-slug="reinforcement-learning">reinforcement learning</a>.</li>
</ul>
</li>
<li>
<p>Running hits the wall.</p>
<ul>
<li>Recognize wall condition</li>
</ul>
</li>
<li>
<p>Do we have different states of each step in short term memory, or is it a gradual differentiation of the identical?</p>
<ul>
<li>If you’re walking down the street and the unification happens at “oh, this is where I’ve been before,” then you’ve had it separately until then.</li>
<li>But what is that unify? If you equate states when given the same input, they don’t become separate in the first place.</li>
<li>If the input is not identical due to some noise or historical information, the UNIFY side will have to allow for some error, and we are back to square one.
<ul>
<li>(2019 comments <img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>)
<ul>
<li>If unification occurs by similarity of input vectors, then it’s just <a href="./k-means" class="internal" data-slug="k-means">k-means</a>.</li>
<li>Unify mechanism different from normal distance is needed here.</li>
<li>It’s “whether or not it’s close to the time we threw out the information on a particular axis.”</li>
<li>It’s a comparison with some of the input vectors set to zero.
- <a href="./dimensionality-reduction-caution" class="internal alias" data-slug="dimensionality-reduction-caution">dimensionality reduction caution</a></li>
</ul>
</li>
</ul>
</li>
<li>Is the Blue Star Algorithm relevant?
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>That possibility cannot be ruled out, especially in the sense of <a href="./Pattern-discovery-from-time-series-input" class="internal alias" data-slug="Pattern-discovery-from-time-series-input">Pattern discovery from time series input</a>.</li>
</ul>
</li>
<li>For this issue to be resolved, it would mean that UNIFY is not a zero-one.</li>
<li>That is, the states are not a discrete set, but are distributed over a continuous space, with distances far and near.
<ul>
<li><a href="./Distributed-representation-of-states" class="internal alias" data-slug="Distributed-representation-of-states">Distributed representation of states</a></li>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/> <a href="./Vectorization-of-states" class="internal alias" data-slug="Vectorization-of-states">Vectorization of states</a></li>
</ul>
</li>
<li>Grid cells just happen to be a grid because the state of the grid is two-dimensional, but it’s a distributed representation.
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>Of course I think so.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Need a mechanism to do this:.</p>
<ul>
<li>Hitting a wall.</li>
<li>I feel discomfort.</li>
<li>As we move forward, we’ll anticipate feeling discomfort again and avoid it.</li>
</ul>
</li>
<li>
<p>Needs a distributed representation of the state and a neural net that takes the next action as an argument and predicts the reward to be obtained</p>
<ul>
<li>This can be learned every step of the way.</li>
<li>Returning a reward with the state and next action as arguments is <a href="./Q-function" class="internal" data-slug="Q-function">Q-function</a>.</li>
</ul>
</li>
<li>
<p>I’m sure my brain simulation will learn to make a U-turn when I hit a wall, for now.</p>
<ul>
<li>If we can understand that “where there was nothing before, there will remain nothing.”</li>
<li>After making a U-turn, he’s going to turn and explore as he sees fit.</li>
</ul>
</li>
<li>
<p>How is a distributed representation of the state created?</p>
</li>
<li>
<p>If the CA3 recurrent is randomly initialized, it would be possible to randomly initialize the variance representation, since we’d get an appropriate value at each step anyway.</p>
</li>
<li>
<p>Is there a grid-cell-like cycle in the time direction as well?</p>
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/> <a href="./Hippocampal-time-compression" class="internal alias" data-slug="Hippocampal-time-compression">Hippocampal time compression</a></li>
</ul>
</li>
<li>
<p>To make the state of nearness near.</p>
</li>
<li>
<p>If a neural net has been trained to return a state from a previously experienced state-input pair, where the input returns the state for the argument, then it seems possible for the recurrent net to relearn, but in that case the firing pattern is just an ID, not a distributed representation</p>
<ul>
<li>Is that still okay?</li>
<li>Then the incoming states of similar inputs come to fire together, which is redundant, so compressing them allows unification.</li>
<li>I think I can do it if I put sparse constraints on it.</li>
<li>Then it’s going to be a place cell thing.</li>
</ul>
</li>
<li>
<p>If this is a distributed representation with sparse constraints, would it be a grid?</p>
<ul>
<li>Not so easy.</li>
</ul>
</li>
<li>
<p>It is a good thing if a place cell is created.</p>
</li>
<li>
<p>Once we have a model that can produce place cells, let’s feed them natural language as input.</p>
<ul>
<li>I hope the rat has a neural net that receives the state and returns the next action, and that guy is initialized randomly at first.</li>
<li>Then you’re repeating the same behavior.</li>
<li>How to do something like UCB1 with neural nets</li>
<li>A mechanism that narrows when nothing happens.
<ul>
<li><a href="./Thompson-sampling" class="internal alias" data-slug="Thompson-sampling">Thompson sampling</a> or</li>
</ul>
</li>
<li>No, I guess not.</li>
<li>Need a neural net that can express confidence</li>
<li>Is it enough to have lots of outputs?</li>
<li>There is a randomly initialized net that takes a state and returns an action, with several outputs for each action and a majority vote.
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>If there are multiple identical networks with different random initial values, the outputs will be disparate with little learning, and similar outputs will emerge as the learning progresses, thus expressing “<a href="./degree-of-confidence" class="internal alias" data-slug="degree-of-confidence">degree of confidence</a>”.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>The condition looks better coming in before the sparse.</p>
</li>
<li>
<p>Actually, maybe the behavior is a distributed expression, too.</p>
</li>
<li>
<p>Realization of delayed compensation</p>
<ul>
<li>If we used rewards directly to learn this state-action mapping, we would not be able to deal with delayed rewards, so life began to predict and estimate the value of the next state sometime in the future.</li>
<li>We have to do something here, because if the rats don’t run around, the map won’t be made, and if they don’t understand the delayed rewards, they won’t run around.</li>
<li>All you need is a lot of output from a neural net that takes a state and an action and returns a value.</li>
<li>The distribution of output is uniformly distributed since it is initially initialized at random
<ul>
<li>Sharp distribution as learning is repeated.</li>
</ul>
</li>
<li>If we choose the largest randomly selected one of these outputs, it is roughly Thompson sampling</li>
<li>Oh, you don’t have to work hard to realize Thompson sampling or UCB1, or even epsilon greedy.</li>
<li>If only we could have introduced neurons that randomly select</li>
</ul>
</li>
<li>
<p>After sparsing the state, it’s normal reinforcement learning, so we should implement it normally and first test the expectation that nets from input to state and recurrent nets and sparsing will produce place cells.</p>
</li>
<li>
<p>So far, we have created cells that respond to specific locations, but their positioning is still in question.</p>
</li>
<li>
<p>I wonder if it learns state transitions even after sparsing…</p>
</li>
<li>
<p>There is no recurrent over here, though.</p>
</li>
<li>
<p>Hourly average.</p>
</li>
<li>
<p>How to make states that transition closer and states that do not transition farther away</p>
<ul>
<li>Stimulated neurons remain active longer than the temporal resolution of other neurons</li>
<li>Its neurons act as a compression of the time axis.</li>
</ul>
</li>
<li>
<p>The orientation cells are born here because “running straight” is a frequent, natural behavior of rodents.</p>
</li>
<li>
<p>I hope the duration of the firing is determined by some concentration, and that there is a concentration gradient along the hippocampus.</p>
</li>
<li>
<p>I don’t know why it’s on the grid…</p>
<ul>
<li>Surprisingly, you might be able to get a grid just by compressing time averaging into two dimensions.
<ul>
<li>My guess is negative.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>It seems to be born out of the behavior patterns of rats.</p>
</li>
<li>
<p>My interest is in natural language anyway, so I don’t care about the grid.</p>
<ul>
<li>It would be amazing if we could inadvertently grid</li>
</ul>
</li>
<li>
<p>What happens when you create and nurture an <a href="./intelligence-form" class="internal alias" data-slug="intelligence-form">intelligence form</a> that can only read and write?</p>
</li>
</ul>
<hr/>
<p>This page is auto-translated from <a href="https://scrapbox.io/nishio/%E8%AA%AD%E6%9B%B8%E3%82%92%E6%94%AF%E6%8F%B4%E3%81%99%E3%82%8B%E3%83%9C%E3%83%83%E3%83%88" class="external">/nishio/読書を支援するボット<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at <a href="https://twitter.com/nishio_en" class="external">@nishio_en<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. I’m very happy to spread my thought to non-Japanese readers.</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="./Don't-aim-to-encounter-the-unknown." class="internal">Don't aim to encounter the unknown.</a></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.4.0</a> © 2024</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="./postscript.js" type="module"></script></html>