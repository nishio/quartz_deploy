<!DOCTYPE html>
<html lang="en"><head><title>Hatena2011-12-22</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="ü™¥ Quartz 4.0"/><meta property="og:title" content="Hatena2011-12-22"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Hatena2011-12-22"/><meta name="twitter:description" content="hatena &lt;body> *1324561616* Learning natural language with hidden Markov models I used a hidden Markov model to train just under 10,000 posts on the company bulletin board ..."/><meta property="og:description" content="hatena &lt;body> *1324561616* Learning natural language with hidden Markov models I used a hidden Markov model to train just under 10,000 posts on the company bulletin board ..."/><meta property="og:image:type" content="image/webp"/><meta property="og:image:alt" content="hatena &lt;body> *1324561616* Learning natural language with hidden Markov models I used a hidden Markov model to train just under 10,000 posts on the company bulletin board ..."/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:width" content="1200"/><meta property="og:height" content="630"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https:/quartz.jzhao.xyz/Hatena2011-12-22"/><meta property="twitter:url" content="https:/quartz.jzhao.xyz/Hatena2011-12-22"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="hatena &lt;body> *1324561616* Learning natural language with hidden Markov models I used a hidden Markov model to train just under 10,000 posts on the company bulletin board ..."/><meta name="generator" content="Quartz"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("./static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Hatena2011-12-22"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href=".">ü™¥ Quartz 4.0</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="./">Home</a><p> ‚ùØ </p></div><div class="breadcrumb-element"><a href>Hatena2011-12-22</a></div></nav><h1 class="article-title">Hatena2011-12-22</h1><p show-comma="true" class="content-meta"><span>Dec 22, 2011</span><span>7 min read</span></p></div></div><article class="popover-hint"><p>hatena</p>
<pre><code>&lt;body>
*1324561616* Learning natural language with hidden Markov models
I used a hidden Markov model to train just under 10,000 posts on the company bulletin board.
[f:id:nishiohirokazu:20111222224531g:image]
First, let's talk about initial values. The transition probability is approximately a diagonal matrix. If the probability is 0, the transition is not interesting, so I normalized the diagonal component to 11, the diagonal one up to 2, and all other components to 1, so that the probabilities are correct. The output probability is random. However, since I wanted to pay attention to the structure of the end of the sentence, I doubled the output probability of the punctuation &quot;. is doubled only for the last state because we wanted to focus on the structure of the end of the sentence.
[f:id:nishiohirokazu:20111222232854p:image]
The leftmost figure shows the size of the transition probability matrix, represented by the size of the black square. Since the values are normalized by the maximum and minimum values, the area where the black square is not visible does not mean that the probability is zero, but rather that it is a small value. The center panel is a twice enlarged version of the display. The center panel is a double magnified image of the display, since it is difficult to see the smallest probability values. The red color means that the probability exceeds 1 as a result of doubling the value. The rightmost panel has a black, red, green, and white scale. Well, this was the first one I made, but I thought it would offend the red-green color-blind people.

The list of characters below shows the 20 characters that each state outputs, in order of frequency.

The following is a one-step study.
[f:id:nishiohirokazu:20111222233403p:image]
Roughly all states now output the average frequency of character occurrence. (UNK)(RET)(SP) corresponds to &quot;unknown character,&quot; &quot;line feed,&quot; and &quot;half-width space,&quot; respectively. Oh, I forgot to mention that only the top 90% of the characters are recognized as each character in order of frequency of occurrence, and the rest are treated as &quot;unknown characters. You can see that there are subtle differences in the output characters for each state depending on the connection of the transition matrices and other factors.

[f:id:nishiohirokazu:20111222233938p:image]
It can be observed that clusters centering on English letters and clusters centering on hiragana are beginning to differentiate in this area.
[f:id:nishiohirokazu:20111222233937p:image]
The transition probability gradually flattens out and then rises all the way up here.
[f:id:nishiohirokazu:20111222233936p:image]
And we are looking for appropriate convergence patterns.
[f:id:nishiohirokazu:20111222233935p:image]
[f:id:nishiohirokazu:20111222233934p:image]
Among s2, s3, and s4, which were mainly English letters, only s4 leaps to second place in frequency of occurrence of 0.

It's hard to go through them one at a time, so let's go through them all at once. The next step is after the completion of 20 studies.
[f:id:nishiohirokazu:20111222234328p:image]
It is observed that s0 is katakana, s1 is uppercase English letters, s2 is symbols, s3 is lowercase English letters, s4 is numbers, and s5 is hiragana and punctuation marks. What is important to note is that the classification of character types was made by simply feeding in the data from the bulletin board posts, without providing any information such as the sequence of code points for each character. By summarizing the data, we were able to discover six clusters.

Now, since I want to observe the punctuation, I will duplicate s5 to make s6. In the animation, the red area widens for a moment, but this is due to the fact that the transition matrix is normalized by adding 0.1 to the values other than the one to be duplicated at the time of duplication. If you think about it, you don't have to go that far, so you can put 1.0 / K in the newly added right-most column, but for now, the probability values are averaged out because 0.1 is added to all of them. Well, it's okay, because the next step will separate the large and small again soon.

Here is what was learned 20 more times after that.
[f:id:nishiohirokazu:20111222235114p:image]
The s6 is in the state of being a two-top for line breaks and punctuation. Moreover, the transition to s0 is rather strong. s0 is supposed to collect characters close to the &quot;beginning of the sentence&quot; because it has a bias such that the first character of a bulletin board post is always s0. So, I discovered that when the punctuation mark appears, the transition to the beginning of the sentence is similar to that of the beginning of the message board. I see. Well, let's try to duplicate s6 again. Here is what I learned 40 times after that.

[f:id:nishiohirokazu:20111222235751p:image]
The punctuation dances to the top in s6. Not only that, but &quot;...? ...?&quot; which are very much like punctuation marks. The transition probability shows that s6 does not stay to itself but immediately transitions to s7. The punctuation &quot;. is learning my writing pattern, that is, I often do not write several punctuation &quot;.

I didn't notice this when I was learning this, but a certain distinctive character appears in s7. What is it?

After this, we duplicated s6, did the learning, duplicated the clusters with a lot of punctuation, and repeated several times, and here is what we got:

[f:id:nishiohirokazu:20111223000315p:image]
I can still see that s10 is a punctuation cluster and the transition from there is to s7. I can still see that there are a lot of line breaks and unknown characters, but what is that next full-width white space and middle black and white? Ah, well, that's the thing! When I do bullet points in a plain text environment where I can't use HTML tags, I use curly braces and full-width spaces. I guess I've learned that!

If you look at s9, which has many transitions to s10, you will see many characters like &quot;„Åß„Åô&quot;, &quot;„Åã„Å™„ÅÅ&quot;, and &quot;„Çì„Å†&quot;. It's the end of a sentence.
I wonder what s8 is. s5 has many transitions to s5. s5...has the most unknown characters, but the rest are &quot;ga&quot;, &quot;de&quot;, and &quot;te-no-ha&quot;, so it seems to be the main part. s7 contains bullet points and so on that precede sentences, and there are many transitions from there. s7 is an ornament before a sentence? But it's puzzling that it contains a lot of closing brackets. I guess there weren't enough states to split it up.

What would be interesting to do after this? Maybe add one more state that is flatly connected from all the states and initialized to the current probability and observe what role it starts to play. It will be interesting to see if it learns to distinguish between open and close brackets, or if it progresses to more subdivision around numbers and alphabets....

As usual, this image is licensed under CC-BY3.0, so you can use it freely as long as you indicate the author's name. However, it may not be as easy to use as the previous one (&lt;a href='http://d.hatena.ne.jp/nishiohirokazu/20111122/1321925412'>Estimating clusters in a mixed Gaussian distribution model using the k-means method, EM algorithm, and variational Bayesian method&lt;/a> respectively). but it is not as easy to use as the previous one.
&lt;/body>
</code></pre>
<h2 id="hatena-diary-2011-12-22"><a href="https://nishiohirokazu.hatenadiary.org/archive/2011/12/22" class="external">Hatena Diary 2011-12-22<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#hatena-diary-2011-12-22" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>This page is auto-translated from <a href="https://scrapbox.io/nishio/Hatena2011-12-22" class="external">/nishio/Hatena2011-12-22<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at <a href="https://twitter.com/nishio_en" class="external">@nishio_en<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. I‚Äôm very happy to spread my thought to non-Japanese readers.</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.4.0</a> ¬© 2024</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="./postscript.js" type="module"></script></html>