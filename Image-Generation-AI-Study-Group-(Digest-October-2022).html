<!DOCTYPE html>
<html lang="en"><head><title>Image Generation AI Study Group (Digest October 2022)</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="ü™¥ Quartz 4.0"/><meta property="og:title" content="Image Generation AI Study Group (Digest October 2022)"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Image Generation AI Study Group (Digest October 2022)"/><meta name="twitter:description" content="Cybozu Labs Study Session 2022-11-11 In the month since the last Stable Diffusion Study Group on 9/30, there have been intense story developments around image generation AI ..."/><meta property="og:description" content="Cybozu Labs Study Session 2022-11-11 In the month since the last Stable Diffusion Study Group on 9/30, there have been intense story developments around image generation AI ..."/><meta property="og:image:type" content="image/webp"/><meta property="og:image:alt" content="Cybozu Labs Study Session 2022-11-11 In the month since the last Stable Diffusion Study Group on 9/30, there have been intense story developments around image generation AI ..."/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:width" content="1200"/><meta property="og:height" content="630"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https:/quartz.jzhao.xyz/Image-Generation-AI-Study-Group-(Digest-October-2022)"/><meta property="twitter:url" content="https:/quartz.jzhao.xyz/Image-Generation-AI-Study-Group-(Digest-October-2022)"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="Cybozu Labs Study Session 2022-11-11 In the month since the last Stable Diffusion Study Group on 9/30, there have been intense story developments around image generation AI ..."/><meta name="generator" content="Quartz"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("./static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Image-Generation-AI-Study-Group-(Digest-October-2022)"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href=".">ü™¥ Quartz 4.0</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="./">Home</a><p> ‚ùØ </p></div><div class="breadcrumb-element"><a href>Image Generation AI Study Group (Digest October 2022)</a></div></nav><h1 class="article-title">Image Generation AI Study Group (Digest October 2022)</h1><p show-comma="true" class="content-meta"><span>Nov 14, 2022</span><span>24 min read</span></p><ul class="tags"><li><a href="./tags/dreambooth" class="internal tag-link">dreambooth</a></li></ul></div></div><article class="popover-hint"><p>Cybozu Labs Study Session 2022-11-11</p>
<ul>
<li>In the month since the last <a href="./Stable-Diffusion-Study-Group" class="internal alias" data-slug="Stable-Diffusion-Study-Group">Stable Diffusion Study Group</a> on 9/30, there have been intense story developments around image generation AI.
<ul>
<li>I‚Äôll recap it in this digest and explain how <a href="./Imagic" class="internal" data-slug="Imagic">Imagic</a> and <a href="./Aesthetic-Gradients" class="internal alias" data-slug="Aesthetic-Gradients">Aesthetic Gradients</a> work in the remaining time.</li>
</ul>
</li>
<li>10/3 NovelAI, a provider of novel creation AI services, releases paid image generation AI NovelAIDiffusion
<ul>
<li>Animation picture specializing in high quality and noisy</li>
<li>Capable of learning and generating images with arbitrary aspect ratios, which was not possible with Stable Diffusion</li>
<li>In the Japanese-speaking world, people became angry because the study source was an unauthorized reproduction site.</li>
</ul>
</li>
<li>10/7 NovelAIDiffusion source code and models leaked and shared via Torrent</li>
<li>10/12 NovelAI announces that the number of images generated has exceeded 30 million in the first 10 days since its release.
<ul>
<li>Roughly speaking, the image is of sales of 3 million yen per day.</li>
</ul>
</li>
<li>10/17 NovelAI Prompt Manual ‚ÄúCode of Elements‚Äù in Chinese
<ul>
<li>Sideline evidence that the use of NovelAI‚Äôs spill model is major in Chinese-speaking countries.</li>
</ul>
</li>
<li>10/18 Imagic is the talk of the town.
<ul>
<li>Some say it‚Äôs very useful and can be used properly, others say it‚Äôs not quite as useful as expected.</li>
<li>I‚Äôm the latter, but this could be ‚ÄúI just don‚Äôt understand how to use it well‚Äù.</li>
</ul>
</li>
<li>10/20 Stable Diffusion 1.5 is released by Runway, not by Stability AI, which released 1.4; Stability AI files for temporary removal, but later withdraws it.</li>
<li>10/21 Stability AI, (in a big hurry?) Released new VAE, one that improves eye and face decoding</li>
<li>10/22 Strange people came to the home of a person who was sending out information related to NovelAI in Japanese, resulting in police action</li>
<li>11/3 ‚ÄúNovelAI Aspect Ratio Bucketing‚Äù released under MIT license</li>
</ul>
<p>NovelAIDiffusion Release</p>
<ul>
<li>NovelAI, a provider of novel creation AI services, releases NovelAIDiffusion, a paid image generation AI</li>
<li>In Stable Diffusion, the prompt was censored at 77 tokens, but in NovelAIDiffusion, it triples to 231 tokens</li>
<li>Stable Diffusion used to crop the training data into a square, but thanks to NovelAI‚Äôs ingenuity, it is now possible to train and generate data in any aspect ratio.
<ul>
<li>Unlike university laboratories that aim to publish papers, this is a service of a for-profit company, so details were not disclosed (they were later made public).</li>
<li>Aspect ratio strongly affects composition.
NAI Curated</li>
</ul>
</li>
</ul>
<pre><code>girl, blue eyes, blue long hair, blue cat ears, chibi
</code></pre>
<pre><code>    - ![image](https://gyazo.com/ec303056563dd0308f6530af5549d053/thumb/1000)![image](https://gyazo.com/a8a40c57789dea0cd4e523c2ed84999c/thumb/1000)![image](https://gyazo.com/e34a2583abf1105d02ba614f08c2877d/thumb/1000)
- The distribution of pictures generated is severely skewed.
    - Prompt &quot;black cat&quot; generates 5 cards and 2 are cat ear girls [[Diary 2022-10-07]].
        - ![image](https://gyazo.com/487f8d241846f06d4a34770a344703db/thumb/1000)![image](https://gyazo.com/65e72a194351fed5c17fc59eb07d4961/thumb/1000)
        - I almost blew tea when I saw the first one with &quot;Let's just put in 'black cat' for comparison with Stable Diffusion.
    - SNS was abuzz with the overwhelming strength against &quot;anime-style women,&quot; a field in which the company excels.
        - The day's Tweet is recorded here: [https://note.com/yamkaz/n/nbd9a028d625a](https://note.com/yamkaz/n/nbd9a028d625a)
        - Most of the Tweets recorded here are &quot;anime style women&quot;.
        - Specializing and devoting resources to a narrow area of the diverse distribution of &quot;pictures&quot; has led to a watershed in user value in that area.
            - In other areas, the expressive power is reduced, but the extended features have stuck with the customer.
                - [Blue Ocean Strategy.
</code></pre>
<ul>
<li>Controversy erupted over the dataset used for the study.
<ul>
<li>Data from Danbooru, a service that allows volunteers to tag images and search for images by tag, is used for training.</li>
<li>Pros and cons (or at least negative opinions were loudly transmitted on the Japanese-language SNS).
<ul>
<li>Negative: I would not recommend this hotel to anyone.
<ul>
<li>Danbooru is an unauthorized reproduction site and is illegal.</li>
<li>AI trained on illegal data is evil, it is the enemy.</li>
<li>This AI is a paid service, any profit made from it is stolen from us.</li>
</ul>
</li>
</ul>
</li>
<li>By the way, Danbooru itself clearly states the source of the original image and links to it, so it is quite difficult to determine whether this ‚Äúunauthorized reproduction‚Äù is illegal or not.
<ul>
<li><img src="https://gyazo.com/18161037994ef05c4645892aeac400f7/thumb/1000" alt="image"/><img src="https://gyazo.com/f06398234bfe68c7bda296b4c332b7ed/thumb/1000" alt="image"/>
<ul>
<li>It is clearly stated that it was reprinted from Pixiv.</li>
<li><a href="./fair-use" class="internal alias" data-slug="fair-use">fair use</a> is a theory <a href="https://ja.wikipedia.org/wiki/%E3%83%95%E3%82%A7%E3%82%A2%E3%83%A6%E3%83%BC%E3%82%B9" class="external">fair use - Wikipedia<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.</li>
<li>the use of the reproductions will adversely affect the market (including potential markets)‚Äú?</li>
<li>It is difficult to argue that the act of reprinting something that was originally published free of charge with a clear statement of the source is detrimental to the market.</li>
</ul>
</li>
<li>Relation to the display of a duplicate image cache on Google‚Äôs servers in search results in Google searches.
<ul>
<li>If the image is small, it will be ‚ÄúThis is a thumbnail of search results.</li>
<li>If the image is a direct link, it will say ‚ÄúNo duplication.</li>
<li>The image is so large that it seems to divide opinions.</li>
</ul>
</li>
<li>Of course, since this is user-submitted content, some of it may have been uploaded illegally
<ul>
<li>(e.g., reprints from digital comics that are not published online).</li>
<li>However, as long as the service operator is operating in accordance with the <a href="./Digital-Millennium-Copyright-Act" class="internal alias" data-slug="Digital-Millennium-Copyright-Act">Digital Millennium Copyright Act</a> (DMCA), the service operator will not be charged with a crime.
<ul>
<li>Notice and Takedown Procedure (DMCA Notice)</li>
<li>
<blockquote>
<p>If the operator of a website is notified that a copyrighted work has been posted on a website by a third party without the permission of the copyright holder, the website operator is exempt from liability for damages if the work is promptly removed (takedown).</p>
</blockquote>
</li>
</ul>
</li>
<li>Even though Danbooru will be hated by those who are victims of the reprinting of non-public content, as if Danbooru is to blame, legally Danbooru is not to blame, notis responsibility on the part of the victim.</li>
</ul>
</li>
<li>10/5 <a href="https://gigazine.net/news/20221005-novelai-danbooru/" class="external">Danbooru official, the source of the training, released a statement about NovelAI, an automatic illustration generation AI - GIGAZINE<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>
<ul>
<li>Roughly ‚ÄúTell NovelAI about AI, we have nothing to do with it. If we have proof that you are the copyright holder, we will agree to remove it.‚Äù</li>
<li>From a DMCA perspective, the burden of proof is on the party claiming unauthorized reproduction, so I would say so.</li>
</ul>
</li>
</ul>
</li>
<li>The world says, ‚ÄúWhat‚Äôs wrong with using Danbooru?‚Äù in the world.
<ul>
<li>
<blockquote>
<p><a href="https://twitter.com/MutedGrass/status/1576861235458424833?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1576861235458424833%7Ctwgr%5E669f7044c21e6b7686422c72bc25ca959dc4862c%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fnote.com%2Fyamkaz%2Fn%2Fnbd9a028d625a" class="external">@MutedGrass<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>:</p>
</blockquote>
</li>
<li>
<blockquote>
<p>/StableDiffusion‚Ä¶LAION 5B has Danbooru‚Äôs image URL</p>
</blockquote>
</li>
<li>
<blockquote>
<p>/WaifuDiffusion‚Ä¶Danbooru 2021 dataset use clearly stated.</p>
</blockquote>
</li>
<li>
<blockquote>
<p>/NovelAI‚Ä¶stated Danbooru use.</p>
</blockquote>
</li>
<li>
<blockquote>
<p>Mid Journey‚Ä¶collaborate with WaifuLabs to use Safebooru-derived data (planned)</p>
</blockquote>
</li>
<li>
<blockquote>
<p>In other words, everyone is using Danbooru! that means everyone is using Danbooru!</p>
</blockquote>
</li>
<li>In other words, the reaction of the Japanese-speaking world to NovelAI‚Äôs use of Danbooru is the <a href="./group-polarization" class="internal alias" data-slug="group-polarization">group polarization</a> phenomenon.
<ul>
<li>Opposition shouted louder, so neutral - proponents shut up for fear of damage.</li>
<li>Heard on multiple channels that ‚Äúwe‚Äôre trying, but we‚Äôre not disseminating information.‚Äù
- <a href="./New-Technology-and-Publication-Bias" class="internal alias" data-slug="New-Technology-and-Publication-Bias">New Technology and Publication Bias</a></li>
<li>Some people have advised me to refrain from expressing logically correct opinions because ‚Äúeven logically correct opinions can get you tangled up with crazy people.‚Äù</li>
</ul>
</li>
<li>10/22 <a href="https://note.com/852wa/n/nee6da6dd577b" class="external">regarding house convexity | episode 852 |note<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>
<ul>
<li>A case of a strange person coming to the home of a person who was actively disseminating information.</li>
</ul>
</li>
<li>10/29 <a href="https://anond.hatelabo.jp/20221029165248" class="external">AI painter started but numbers are egregious<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>
<ul>
<li>
<blockquote>
<p>On twitter, there are many people who say, ‚ÄúI hide pictures with AI tags. Some people say, ‚ÄúAfter all, pictures must be drawn by humans,‚Äù but this was just ‚Äúthe opinion of a vocal minority,‚Äù a fact that I felt clearly when I looked at the numbers of my own account on pixiv.</p>
</blockquote>
</li>
</ul>
</li>
<li>10/31 <a href="https://www.pixiv.net/info.php?id=8728" class="external">pixiv News - Features for handling AI-generated works have been released<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.
<ul>
<li>Pixiv has landed on the ‚Äúdon‚Äôt eliminate AI-generated works, but let them segregate themselves with separate rankings.</li>
<li>Meanwhile, Danbooru has banned the submission of AI works as of confirmation on 11/10.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>NovelAI Leakage</p>
<ul>
<li>10/7 NovelAIDiffusion source code and models leaked and shared via Torrent
<ul>
<li>Only 4 days after release, w</li>
</ul>
</li>
<li>10/12 NovelAI announces that the number of images generated has exceeded 30 million in the first 10 days since its release.
<ul>
<li><img src="https://gyazo.com/d969383120e2b7534cfa13e28d8e4fda/thumb/1000" alt="image"/></li>
<li><img src="https://gyazo.com/009042f555c1f5f34189e86c14faade1/thumb/1000" alt="image"/></li>
<li>The smallest of the preset sizes is 512x512, and if you produce 4 pieces with default parameters, it‚Äôs 20anlas, so about 2000 pieces for $11.
<ul>
<li>(The default parameter was changed after this to 16anlas.)</li>
<li>It‚Äôs probably used for higher resolution and such, so roughly speaking, it‚Äôs about a penny a piece.</li>
<li>Roughly speaking, the image is of sales of 3 million yen per day.</li>
</ul>
</li>
</ul>
</li>
<li>10/17 NovelAI Prompt Manual ‚ÄúCode of Elements‚Äù in Chinese
<ul>
<li><img src="https://gyazo.com/f2732d53db16958208ab0c02fe9369cf/thumb/1000" alt="image"/><a href="https://docs.qq.com/doc/DWHl3am5Zb05QbGVs" class="external">docs<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>
<blockquote>
<p><img src="https://gyazo.com/d6123280b089eedc35c54fa78baf0c58/thumb/1000" alt="image"/></p>
</blockquote>
</li>
<li>
<blockquote>
<p>Easily done <a href="./Diary-2022-10-17" class="internal alias" data-slug="Diary-2022-10-17">Diary 2022-10-17</a>.</p>
</blockquote>
</li>
<li><img src="https://gyazo.com/7ea88e2f341de202cf6061ce045bb6a3/thumb/1000" alt="image"/>
<ul>
<li>This round bracket used for vector emphasis in tokens, does not work for NovelAI‚Äôs service.
- <a href="./Using-round-brackets-in-NovelAI-is-pointless." class="internal alias" data-slug="Using-round-brackets-in-NovelAI-is-pointless.">Using round brackets in NovelAI is pointless.</a>
<ul>
<li>The round brackets are the de facto standard AUTOMATIC1111/stable-diffusion-webui functionality for running Stable Diffusion locally</li>
<li>In other words, this is a major proof that in Chinese-speaking countries, the local runoff model is used instead of NovelAI‚Äôs service.</li>
<li>The use of leaked models, some people say something like ‚Äúit‚Äôs illegal, so don‚Äôt do it‚Äù in Japan, but what kind of law does it violate? I‚Äôm not sure.
<ul>
<li>In Japanese law, is it Article 2, Paragraph 1, Item 5 of the Unfair Competition Prevention Law?</li>
<li>
<ul>
<li>acquires a trade secret or uses or discloses a trade secret without knowledge with knowledge that an act of wrongful acquisition of a trade secret has intervened or without knowledge due to gross negligence</li>
</ul>
</li>
<li>I think NovelAI was in Delaware, maybe there is a similar law.</li>
<li>Well, even if there were, it would be hard to sue Chinese users.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Without the spill, the Elements Code would not have been created.
<ul>
<li>Time may have to tell if the leak was a bad thing for NovelAI.</li>
</ul>
</li>
</ul>
<p>Imagic</p>
<ul>
<li>10/18 Imagic is the talk of the town.</li>
<li>
<blockquote>
<p><a href="https://twitter.com/AbermanKfir/status/1582218210547757056?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1582218210547757056%7Ctwgr%5E93858c6e839b7b851b90c90aa0cade388dbeadde%7Ctwcon%5Es1_c10&amp;ref_url=https%3A%2F%2Fnote.com%2Fyamkaz%2Fn%2Fn55c3ac365f34" class="external">@AbermanKfir<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>: The combination of<a href="./tags/dreambooth" class="tag-link internal alias" data-slug="tags/dreambooth">dreambooth</a> and embedding optimization paves the way to new image editing capabilities. Love it. Congrats on this nice work!</p>
</blockquote>
<ul>
<li><a href="./DreamBooth" class="internal" data-slug="DreamBooth">DreamBooth</a> + <a href="./embedding-optimization" class="internal alias" data-slug="embedding-optimization">embedding optimization</a></li>
</ul>
</li>
<li><img src="https://gyazo.com/c4b331f315d8d71419e2fb58ada3a5c7/thumb/1000" alt="image"/></li>
<li>Some say it‚Äôs very useful and can be used properly, others say it‚Äôs not quite as useful as expected.
<ul>
<li>I‚Äôm the latter, but this could be ‚ÄúI just don‚Äôt understand how to use it well‚Äù.</li>
<li><img src="https://gyazo.com/905cdfcbae8f2199b00fbb470fd7db67/thumb/1000" alt="image"/> + ‚Äúa woman wearing black suit‚Äù = <img src="https://gyazo.com/a0f9ca631a50c9c47882cdb6ac64cb05/thumb/1000" alt="image"/>
<ul>
<li><a href="https://birdmanikioishota.blog.fc2.com/blog-entry-12.html" class="external">Understanding Imagic - Hoge Hoge<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>The remarkable thing is that ‚Äúthe face is preserved to the extent that it would not be out of place if people said it was the same person.‚Äù</li>
</ul>
</li>
<li>
<blockquote>
<p><a href="https://twitter.com/npaka123/status/1582696004414939137?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1582696004414939137%7Ctwgr%5E" class="external">80908a8905a2b157f0902fc3a878d9d3d3b5735e|twcon^s1_c10&amp;ref_url=https%3A%2F%2Fnote.com%2Fyamkaz%2Fn%2Fn7a7394323358 @npaka123<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>: I was trying to get my cat Imagic Stable Diffusion, but the cat didn‚Äôt change and moved to a bedroom-like place.</p>
</blockquote>
</li>
<li>
<blockquote>
<p><img src="https://pbs.twimg.com/media/FfbbX54VsAAUp2Z.jpg" alt="image"/><img src="https://pbs.twimg.com/media/FfbbZARVUA0ydqG.png" alt="image"/></p>
</blockquote>
<ul>
<li>The cat is saved.</li>
</ul>
</li>
<li><a href="./Imagic-2022-10-31" class="internal alias" data-slug="Imagic-2022-10-31">Imagic 2022-10-31</a>
<ul>
<li><img src="https://gyazo.com/6b7beb6c41765ff93c1bdede39f5d14a/thumb/1000" alt="image"/><img src="https://gyazo.com/ece7c6d8f55c16a421c86b70afdf5204/thumb/1000" alt="image"/></li>
<li>Prompt with flower.</li>
<li>The default strength is 0.9, but that didn‚Äôt change at all, so I increased it and it flowered.</li>
</ul>
</li>
<li><a href="https://scrapbox.io/villagepump/2022/10/30#635eaefae2dacc0000f46cc2" class="external">/villagepump/2022/10/30#635eaefae2dacc0000f46cc2<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>
<ul>
<li><img src="https://gyazo.com/bdd1d9b05d5d826c4b5b623fdd88fb70/thumb/1000" alt="image"/></li>
</ul>
</li>
<li>Opinion that it is easier to function in live action <a href="https://scrapbox.io/villagepump/2022/11/01#6360b5eee2dacc0000329928" class="external">/villagepump/2022/11/01#6360b5eee2dacc0000329928<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>.
<ul>
<li>Well, surely it‚Äôs more amazing that something generated by a NovelAI model is Imagic with a Stable Diffusion model and working properly?</li>
<li>
<blockquote>
<p><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>It‚Äôs about two orders of magnitude more time-consuming than img2img, but it doesn‚Äôt maintain the original picture that well.</p>
</blockquote>
<ul>
<li>Different models, so it‚Äôs normal not to maintain them.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Principles and other stories were added at the end of this presentation.</li>
</ul>
<p>Stable Diffusion 1.5</p>
<ul>
<li>10/20 Stable Diffusion 1.5 is released by Runway, not by Stability AI, which released 1.4.
<ul>
<li>Stability AI applies for temporary removal, but later withdraws</li>
<li>I‚Äôm guessing it was a mistake on Stability AI‚Äôs part to not properly grasp the scope of the rights to the joint research work product.
<ul>
<li>The kind of thing where you thought you had exclusive rights, but you didn‚Äôt.</li>
</ul>
</li>
<li>On Runway‚Äôs part, it‚Äôs reasonable to release it because it‚Äôs a chance to raise awareness.
<ul>
<li>I think a lot of people are starting to know and be aware of Runway because of this, myself included.</li>
</ul>
</li>
<li>consideration
<ul>
<li><a href="https://note.com/yamkaz/n/n165fa3922570" class="external">https://note.com/yamkaz/n/n165fa3922570<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>Stability AI side wants to promote NSFW countermeasures, but also wants to release models without countermeasures, so Runway released them.</li>
<li>I think it‚Äôs too much to ask.
<ul>
<li>Runway is also a private company, so there is no incentive to take on risk.</li>
<li>If that‚Äôs the purpose, why not just leak it with the same pose as NovelAI, that it was leaked by an anonymous hacker attack?</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>10/21 Stability AI, (in a big hurry?) Released new VAE, one that improves eye and face decoding
<ul>
<li>I interpret this as ‚Äúwe‚Äôre not ready to release 1.6, but we don‚Äôt want Runway to stay up-to-date for too long, so let‚Äôs release what we can as soon as we can.</li>
<li>Combining the 1.5 model from Runway with the VAE from Stability AI at hand, ‚ÄúThe facial expressions are so much better!‚Äù some people are saying.</li>
<li>is personally distancing myself from the feeling that ‚ÄúDEPENDENCY HELL is about to start‚Ä¶‚Äù</li>
</ul>
</li>
<li>Runway: AI Magic Tool
<ul>
<li>We provide a variety of useful services centered on video editing.</li>
<li>Infinite Image
<ul>
<li>So-called outpainting</li>
<li><img src="https://gyazo.com/e2ba3a5007a13db2ed0b672d38e628be/thumb/1000" alt="image"/><img src="https://gyazo.com/2f924b10840f6848a8abba45616879c5/thumb/1000" alt="image"/></li>
<li>Can‚Äôt you tell it‚Äôs a composite from a distance?</li>
<li>Specify the area you want to composite.
<ul>
<li><img src="https://gyazo.com/afd13ca995fbfe6726ee3e8be4d36a03/thumb/1000" alt="image"/></li>
</ul>
</li>
<li>Press the generate button to make 4 sheets and choose one.
<ul>
<li><img src="https://gyazo.com/ff857109afe0720fb5009cd51f811f71/thumb/1000" alt="image"/><img src="https://gyazo.com/24c6ac45328412eb8f770c16c801ea99/thumb/1000" alt="image"/><img src="https://gyazo.com/27b221e146cc92face56920c611b8243/thumb/1000" alt="image"/><img src="https://gyazo.com/8b9aec27e53f5feed3f48a488f19017f/thumb/1000" alt="image"/></li>
</ul>
</li>
<li>He doesn‚Äôt seem to be very good at cartoon style.
<ul>
<li><img src="https://gyazo.com/6b49334374d1adfffa61f036768f12ca/thumb/1000" alt="image"/>‚Üí<img src="https://gyazo.com/65af1d0f78bebbb48de566a423ceb535/thumb/1000" alt="image"/></li>
<li>NovelAI img2img Noise 0 Strength 0.5</li>
<li>Outpainting does not change the original image (facial expressions and so on).</li>
<li>img2img is roughly the same, but the details change.</li>
</ul>
</li>
</ul>
</li>
<li>Erase and Replace
<ul>
<li>So-called inpainting</li>
<li>Probably because it fills in noise <a href="./Runway's-inpainting-is-excellent." class="internal alias" data-slug="Runway's-inpainting-is-excellent.">Runway‚Äôs inpainting is excellent.</a>
<ul>
<li>Tends to make mysterious things appear in the erased area</li>
</ul>
</li>
</ul>
</li>
<li>Other assortments include object tracking for video and noise reduction for audio.</li>
</ul>
</li>
</ul>
<p>Technology behind NovelAIDiffusion</p>
<ul>
<li>10/11 <a href="https://blog.novelai.net/novelai-improvements-on-stable-diffusion-e10d38db82ac" class="external">NovelAI Improvements on Stable Diffusion | by NovelAI | Oct, 2022 | Medium<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>10/22 <a href="https://blog.novelai.net/the-magic-behind-novelaidiffusion-b4797e0d27b2" class="external">The Magic behind NovelAIDiffusion | by NovelAI | Oct, 2022 | Medium<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>11/3 ‚Äú<a href="https://github.com/NovelAI/novelai-aspect-ratio-bucketing" class="external">NovelAI Aspect Ratio Bucketing<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>‚Äù published under MIT license.</li>
<li>On 10/11, I wrote a technically pointed talk, but the world fundamentally doesn‚Äôt understand how image generation AI works, and they keep saying things like, ‚ÄúWe‚Äôre just patching images from a database‚Äù and other bullshit, so I said, ‚ÄúNo, we‚Äôre not! I gave a basic explanation on 10/22.</li>
<li>The Magic Behind NovelAIDiffusion (10/22)
<ul>
<li>The original Stable Diffusion was trained on the approximately 150 TB LAION dataset</li>
<li>Fine tuning with 5.3 million records and 6 TB data set.
<ul>
<li>This dataset has detailed text tags</li>
<li>(This is probably Danbooru origin)</li>
</ul>
</li>
<li>The model itself is 1.6 GB and can generate images without reference to external data
<ul>
<li>The size doesn‚Äôt change during learning (= so it doesn‚Äôt remember the image! I‚Äôm just saying)</li>
</ul>
</li>
<li>The model took three months to learn.
<ul>
<li>I don‚Äôt mean that they‚Äôve had the learning process running for 3 months, but that they‚Äôve developed a human to look at the progress along the way and fix the problems - and then repeat the process.</li>
<li>The goal is not to write a paper, but to create a good model and make money through service development, so it‚Äôs okay to do some human trial and error along the way.</li>
</ul>
</li>
<li>The model was trained using eight A100 80GB SXM4 cards linked via NVSwitch and a compute node with 1TB of RAM</li>
</ul>
</li>
<li>Improvement of Stable Diffusion by NovelAI (10/11)
<ul>
<li>Use the hidden state of CLIP‚Äôs penultimate layer
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>penultimate layer is ‚Äúone layer before the final layer‚Äù</li>
<li>Stable Diffusion is a mechanism that uses the hidden state of the final layer of CLIP‚Äôs transformer-based text encoder for guidance on classifier free guidance</li>
<li>Imagen (Saharia et al., 2022) uses the hidden state of the penultimate layer for guidance instead of the hidden state of the final layer.</li>
<li>Discussion in the EleutherAI Discord
<ul>
<li>The final layer of CLIP is prepared to be compressed into a small vector for use in similarity searches</li>
<li>That‚Äôs why the value changes so rapidly.</li>
<li>So using that one previous layer might be better for CFG‚Äôs purposes.</li>
</ul>
</li>
<li>experimental results
<ul>
<li>Using the information from the layer before the final one in Stable Diffusion, I was able to generate an image that matched the prompt, albeit with slightly less accuracy.
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>This is not obvious, because Imagen is not LDM.</li>
</ul>
</li>
<li>Color leaks are more likely to occur when using values from the final layer
<ul>
<li>For example, in ‚ÄúHatsune Miku, red dress‚Äù, the red color of the dress leaks into the color of Miku‚Äôs eyes and hair.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>aspect ratio bucket
<ul>
<li>Existing image generation models have a problem of creating unnatural cropped images.
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>I mean like the lack of a neck in the portrait.</li>
</ul>
</li>
<li>The problem is that these models are trained to produce square images
<ul>
<li>Most training source data is not square</li>
<li>It is desirable to have squares of the same size when processing in batches, so only the center of the original data is extracted for training.</li>
<li>Then, for example, the painting of the ‚Äúknight with crown‚Äù would have its head and legs cut off, and the important crown would be lost.</li>
<li><img src="https://gyazo.com/13aa293442bfe496be831c2c15fd1e69/thumb/1000" alt="image"/></li>
<li>This can produce a human being without a head and legs, or a sword without a handle and tip.</li>
<li>I was trying to create an ancillary service to a novel generating AI service, so this wasn‚Äôt going to work at all.</li>
<li>Also, studying ‚ÄúThe Knight with the Crown‚Äù without the crown is not a good idea because of the mismatch between the text and the content</li>
</ul>
</li>
<li>Tried random crop instead of center crop, but only a slight improvement.</li>
<li>It is easy to train Stable Diffusion at various resolutions, but if the images are of different sizes, they cannot be grouped into batches, so mini-batch regularization is not possible, and the training becomes unstable.</li>
<li>Therefore, we have implemented a batch creation method that allows for the same image size within a batch, but different image sizes for each batch.
<ul>
<li>That‚Äôs aspect ratio bucketing.</li>
</ul>
</li>
<li>To put the algorithm in a nutshell, we have buckets with various aspect ratios, and put the image in the closest aspect ratio.
<ul>
<li>I mean, a little bit of discrepancy is fine.</li>
<li>Random crop for a slight displacement.
<ul>
<li>In most cases, less than 32 pixels need to be removed.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Triple the number of tokens
<ul>
<li>StableDiffusion has up to 77 tokens
<ul>
<li>75 with BOS and EOS</li>
</ul>
</li>
<li>This is a limitation of CLIP</li>
<li>So, round up the prompt to 75, 150, or 225, split it into 75 tokens each, run them through CLIP individually, and combine the vectors</li>
</ul>
</li>
<li>hypernetwork
<ul>
<li>Totally unrelated to the method of the same name proposed in 2016 by Ha et al.
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>You put a name on it without knowing it and covered it up.</li>
</ul>
</li>
<li>Techniques used to correct hidden states using small neural nets from multiple points in a larger network</li>
<li>Can have a greater (and clearer) impact than prompt tuning, and can be attached or detached as a module
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>This means that the ability to provide a switch that can be recognized as a component by the end user and can be attached or detached is an advantage in providing the service.</li>
<li>From our experience in providing novel generation AI to users, we knew that users could understand (and perhaps improve user satisfaction) with regard to providing them with a function switch</li>
<li><img src="https://gyazo.com/4ba1538c98f240966cbc4120215db499/thumb/1000" alt="image"/>
<ul>
<li><a href="https://novelai.net/" class="external">https://novelai.net/<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
</li>
</ul>
</li>
<li>Performance is important
<ul>
<li>Complex architecture increases accuracy, but the resulting slowdown is a major problem in a production environment (when the AI is actually a service that end-users touch).</li>
</ul>
</li>
<li>Initially, we tried to learn embedding (just as we had already tried with the novel generation AI)
<ul>
<li>This is the equivalent of a Texual Inversion</li>
<li>But the model did not generalize well enough.</li>
</ul>
</li>
<li>So we decided to apply hypernetting.
<ul>
<li>After much trial and error, I decided to touch only the K and V parts of the cross-attachment layer.</li>
<li>I won‚Äôt touch the rest of U-net.</li>
<li>Shallow attention layers overlearn, so penalize them during learning.</li>
<li>This method performed as well as or better than fine tuning.
<ul>
<li>Better than fine tuning, especially when data for the target concept is limited</li>
<li>I think it‚Äôs because the hypernet can find sparse regions that match the data in the latent space while the original model is preserved.</li>
<li>Fine tuning with the same data will reduce generalization performance by trying to match a small number of training examples
<ul>
<li><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>Maybe fine tuning of the entire model gives too much freedom and tries to represent the training data a little bit with the overall weights.</li>
<li>By limiting it to adjusting the attention only, the ‚Äúdenoising mechanism by condition vector‚Äù is preserved in a decent state learned with a lot of data, but the input vector to it changes more drastically than the one created by a mere transformer, I thought.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a href="./Imagic" class="internal" data-slug="Imagic">Imagic</a></p>
<ul>
<li>
<p>Mechanism for generating a new image based on a single image and text prompt</p>
</li>
<li>
<p>Input is similar to StableDiffusion‚Äôs img2img, but features the ability to make global pixel changes that img2img does not</p>
</li>
<li>
<p><img src="https://gyazo.com/ded80c6786c8a03b034121c7e7c793ff/thumb/1000" alt="image"/><a href="https://arxiv.org/pdf/2210.09276.pdf" class="external">PDF<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</li>
<li>
<p>How does it work?</p>
</li>
<li>
<p><img src="https://gyazo.com/62f14b20e57c5aea68ef4c72e0269af7/thumb/1000" alt="image"/></p>
<ul>
<li>StableDiffusion is broadly defined as ‚Äútext as input and image as output, learned in text/image pairs.‚Äù
<ul>
<li>But when I opened the box, I found a frozen <a href="./CLIP" class="internal" data-slug="CLIP">CLIP</a> inside.</li>
<li>Text is in the form of embedded vectors before being passed to [LDM</li>
</ul>
</li>
<li>Learning SD is the process of fixing the embedding vector e and output image x and updating the LDM model parameters Œ∏ to minimize the loss L
<ul>
<li><img src="https://gyazo.com/91545820622700ae4ba48769e2685776/thumb/1000" alt="image"/></li>
<li>Imagic is divided into three steps
<ul>
<li>1: First, fix the image and model parameters and optimize the embedding vector
<ul>
<li>Losses here are the same as StableDiffusion, the usual definition of <a href="./DDPM" class="internal" data-slug="DDPM">DDPM</a>.</li>
</ul>
</li>
<li>2: Then fix its embedding vector and optimize the model parameters
<ul>
<li>(An auxiliary network is added to preserve the high-frequency component.)</li>
</ul>
</li>
<li>3: Output image by linear interpolation of e and eopt as input to a new LDM</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>schematic</p>
</li>
<li>
<p>Step 0</p>
<ul>
<li><img src="https://gyazo.com/1328e4f076205f6937e2f97086c19bc5/thumb/1000" alt="image"/></li>
<li>A picture of the cake and the prompt ‚Äúpistachio cake‚Äù are given.</li>
<li>Of course, the image created from the prompt ‚Äúpistachio cake‚Äù is completely different from the image you gave</li>
</ul>
</li>
<li>
<p>Step 1</p>
<ul>
<li><img src="https://gyazo.com/2f6bdf14c72b0623d4f45bd9ac89a664/thumb/1000" alt="image"/></li>
<li>Update the embedding vector e so that the output image is closer to the input image x</li>
<li>I think the images in this diagram are too similar.
<ul>
<li>(The paper does not clearly show the image at this time, it says it looks roughly like this, but it appears to include the influence of the auxiliary model described below.)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Step 2</p>
<ul>
<li><img src="https://gyazo.com/e6794fbaf4198641b9d9acf04de66f94/thumb/1000" alt="image"/></li>
<li>Update model parameter Œ∏ so that the difference between the image generated from eopt and the input image x is reduced by combining auxiliary models</li>
<li>In this case, the auxiliary model part learns and absorbs the details that cannot be represented by LDM, resulting in almost the same image.</li>
<li>Auxiliary models are attached to preserve high-frequency components.
<ul>
<li>The detail is well preserved!‚Äù This is because the network preserves high-frequency components that are not preserved by LDM.</li>
<li>LDM collapses 8x8 pixels into 1 pixel, so the high-frequency component of the information given by the image is lost.</li>
<li>Since the details are restored by the VAE decoder, that does not preserve the face of the individual given by the image. The auxiliary model absorbs the differences.</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Step 3</p>
<ul>
<li><img src="https://gyazo.com/c816daacb1972684d04fc7e8d0bf1cdc/thumb/1000" alt="image"/></li>
<li>He claims that somewhere in the one-dimensional space that this new model generates, ‚Äúthere is something relatively close to what we want to get.
<ul>
<li>The assumption here is that ‚Äúa small space would be considered flat‚Äù.</li>
</ul>
</li>
<li>Argument that a mixing factor of about 0.7 looks good.
<ul>
<li>Well, this is the case with photographs. When I experimented with an animated picture I used in NovelAI, it was almost identical to the original image even at 0.9 (the background color was different).</li>
</ul>
</li>
</ul>
</li>
<li>
<p>consideration</p>
<ul>
<li>Unlike img2img, dynamic changes happen, right? but it does.
<ul>
<li>Input is the same as img2img, but unlike img2img, the given image is not used as the initial value when generating the image later.
<ul>
<li>Generation process is txt2img with auxiliary model</li>
</ul>
</li>
<li>img2img downscales the given image (VAE encode) and paints the picture with it as the initial value.
<ul>
<li>It‚Äôs like a person with bad eyesight drawing a picture while referring to the original picture.</li>
<li>So it‚Äôs absurd to give him a picture of a red dress and ask him to make it blue.</li>
</ul>
</li>
<li>Imagic passes the picture of the red outfit and says, ‚ÄúThis is the picture of the blue outfit.‚Äù
<ul>
<li>The meaning of the word ‚Äúblue‚Äù is moved to ‚Äúred‚Äù by updating the embedding vector.</li>
<li>Update the LDM and auxiliary models to reproduce the ‚Äúpicture of red clothes‚Äù given on it.</li>
<li>And if you change the meaning of the word ‚Äúblue‚Äù back from ‚Äúred‚Äù to ‚Äúblue‚Äù, a ‚Äúpicture of blue clothes‚Äù is generated.</li>
</ul>
</li>
</ul>
</li>
<li>High-frequency components such as the face are preserved because the ‚Äúauxiliary model‚Äù absorbs facial details that would be erased if SD were used normally.</li>
<li>Why is it that even at 0.9, an animated picture is almost identical to the original image (the only difference is the background color)?
<ul>
<li>In the same way in photography, there are cases where the background has changed, not the object to be changed.
<ul>
<li>
<blockquote>
<p><a href="https://twitter.com/npaka123/status/1582696004414939137?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1582696004414939137%7Ctwgr%5E" class="external">80908a8905a2b157f0902fc3a878d9d3d3b5735e|twcon^s1_c10&amp;ref_url=https%3A%2F%2Fnote.com%2Fyamkaz%2Fn%2Fn7a7394323358 @npaka123<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>: I was trying to get my cat Imagic Stable Diffusion, but the cat didn‚Äôt change and moved to a bedroom-like place.</p>
</blockquote>
</li>
<li>
<blockquote>
<p><img src="https://pbs.twimg.com/media/FfbbX54VsAAUp2Z.jpg" alt="image"/><img src="https://pbs.twimg.com/media/FfbbZARVUA0ydqG.png" alt="image"/></p>
</blockquote>
</li>
</ul>
</li>
<li>I think the auxiliary model absorbed most of the object‚Äôs information.
<ul>
<li>Considered ‚Äúinformation that should be kept outside the LDM‚Äù similar to the face</li>
<li>The algorithm doesn‚Äôt determine what is the object it wants to change.</li>
<li>For objects that occupy a large portion of the screen and that SD cannot produce at a high rate because of the prompt, ‚ÄúSD cannot produce, so let‚Äôs use an auxiliary model to absorb it.</li>
</ul>
</li>
<li>Mixing ratioŒ∑ can be changed and tested later.
<ul>
<li>It‚Äôs negligible light here because it‚Äôs just a vector mixing of prompts.</li>
<li>Can be done not only internally but also externally.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Aesthetic Gradient</p>
<ul>
<li>/…õsÀàŒ∏…õt.…™k Àà…°…πe…™di…ônt/</li>
<li><img src="https://gyazo.com/45c6ce5f020171485b09f1355715ece5/thumb/1000" alt="image"/><a href="https://arxiv.org/pdf/2209.12330.pdf" class="external">PDF<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
<li>Research on extracting users‚Äô aesthetic senses and using them for personalization</li>
<li>structure
<ul>
<li>Text prompts are vectorized with CLIP text embedding. c
<ul>
<li>StableDiffusion‚Äôs default would be a 768-dimensional vector.</li>
</ul>
</li>
<li>The average of the user‚Äôs favorite N images corresponding to that prompt, which are vectors in the CLIP image embedding. e</li>
<li>If the vector is normalized, the inner product can be regarded as the similarity.
<ul>
<li>So if we just take e, we can optimize the weight of the text embedding part of CLIP by gradient descent.</li>
<li>A learning rate of 1e-4 should be about 20 steps.</li>
</ul>
</li>
</ul>
</li>
<li>consideration
<ul>
<li>A method to fine-tune what vectors each token is embedded in CLIP</li>
<li>Textual Inversion gives meaning to meaningless tokens, but this method only takes a vector of tokens that already have meaning and moves it slightly in the direction of the user‚Äôs preference.
<ul>
<li>Instead, learning is extremely light.</li>
</ul>
</li>
<li>Another advantage is that unlike TI, this method is essentially multi-word OK.
<ul>
<li>Maybe you could make 2N images from a longer prompt and then make an AG with N of them that you prefer.</li>
<li>For example, in an experiment we did in <a href="./Stable-Diffusion-Embedded-Tensor-Editing" class="internal alias" data-slug="Stable-Diffusion-Embedded-Tensor-Editing">Stable Diffusion Embedded Tensor Editing</a>, a human mixed cat and kitten to create a vector that didn‚Äôt correspond to a word.
<ul>
<li>NovelAI has this functionality as a standard feature, the mixing ratio is determined by human hand.</li>
<li>Aesthetic Gradient can be said to automatically create ‚Äúmoderately mixed vectors‚Äù by learning to select only the ones you like from the images created by CAT and KITTEN.</li>
</ul>
</li>
</ul>
</li>
<li>Another advantage is that images are converted to vectors with CLIP before use, so there is no need for size adjustment.</li>
<li>Since the objective function is that of CLIP, I think that features that are not useful for CLIP‚Äôs task of determining the similarity between images and sentences are likely to be ignored.
<ul>
<li>= Features that don‚Äôt appear in the text are likely to be ignored (there are only 768 dimensions at most).</li>
<li>On the other hand, I think what we want to get from vector adjustment is ‚Äúa preference that cannot be well directed by text,‚Äù so I don‚Äôt know‚Ä¶</li>
<li>I think it‚Äôs useful for ‚Äúit‚Äôs possible to express something in writing, but people don‚Äôt express it well.‚Äù</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Finally.</p>
<ul>
<li>I think DreamBooth is the real deal.
<ul>
<li>It‚Äôs expensive, so there are a lot of papers out there that are like ‚ÄúI made a simpler method!‚Äù but none of them seem to be good enough.</li>
</ul>
</li>
<li>The second is Hypernetwork, but this has not been published and detailed information has not been disclosed, and the situation is as follows: ‚ÄúNovelAI used NovelAIDiffusion with it‚Äù and ‚ÄúThe source code was leaked! The source code has been leaked!
<ul>
<li>This is another way to tweak the Attention, so only what Stable Diffusion can draw originally, it‚Äôs just more controllable since it was learned with Danbooru‚Äôs large number of tags.</li>
<li><img src="https://gyazo.com/1265c59ef89df55dbbf0517191fd4946/thumb/1000" alt="image"/>
<ul>
<li>This is the kind of image</li>
<li>The overall expressive capacity (number of black circles) itself has not changed.</li>
<li>Concentrated black circles in specific painting style areas.</li>
<li>It increased the density of points in the area.
<ul>
<li>If you focus only on that area, it appears to have increased expressive power.
<ul>
<li>[Cognitive Resolution</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Hypernetwork is much smaller than the model of LDM itself, and can be turned on and off as a module, so it may be subdivided into ‚Äúfor people‚Äù and ‚Äúfor backgrounds‚Äù for animated pictures.</li>
</ul>
</li>
</ul>
<hr/>
<p>This page is auto-translated from <a href="https://scrapbox.io/nishio/%E7%94%BB%E5%83%8F%E7%94%9F%E6%88%90AI%E5%8B%89%E5%BC%B7%E4%BC%9A(2022%E5%B9%B410%E6%9C%88%E3%83%80%E3%82%A4%E3%82%B8%E3%82%A7%E3%82%B9%E3%83%88)" class="external">/nishio/ÁîªÂÉèÁîüÊàêAIÂãâÂº∑‰ºö(2022Âπ¥10Êúà„ÉÄ„Ç§„Ç∏„Çß„Çπ„Éà)<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at <a href="https://twitter.com/nishio_en" class="external">@nishio_en<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. I‚Äôm very happy to spread my thought to non-Japanese readers.</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="./Diary-2022-11-11" class="internal">Diary 2022-11-11</a></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.4.0</a> ¬© 2024</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="./postscript.js" type="module"></script></html>