<!DOCTYPE html>
<html lang="en"><head><title>Hatena2014-02-12</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="ğŸª´ Quartz 4.0"/><meta property="og:title" content="Hatena2014-02-12"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Hatena2014-02-12"/><meta name="twitter:description" content="hatena &lt;body> *1392179496*Paper presentation: &quot;Variable-length n-gram language model based on hierarchical Pitman-Yor process&quot; The paper says that existing ..."/><meta property="og:description" content="hatena &lt;body> *1392179496*Paper presentation: &quot;Variable-length n-gram language model based on hierarchical Pitman-Yor process&quot; The paper says that existing ..."/><meta property="og:image:type" content="image/webp"/><meta property="og:image:alt" content="hatena &lt;body> *1392179496*Paper presentation: &quot;Variable-length n-gram language model based on hierarchical Pitman-Yor process&quot; The paper says that existing ..."/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:width" content="1200"/><meta property="og:height" content="630"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https:/quartz.jzhao.xyz/Hatena2014-02-12"/><meta property="twitter:url" content="https:/quartz.jzhao.xyz/Hatena2014-02-12"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="hatena &lt;body> *1392179496*Paper presentation: &quot;Variable-length n-gram language model based on hierarchical Pitman-Yor process&quot; The paper says that existing ..."/><meta name="generator" content="Quartz"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("./static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Hatena2014-02-12"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href=".">ğŸª´ Quartz 4.0</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="./">Home</a><p> â¯ </p></div><div class="breadcrumb-element"><a href>Hatena2014-02-12</a></div></nav><h1 class="article-title">Hatena2014-02-12</h1><p show-comma="true" class="content-meta"><span>Feb 12, 2014</span><span>7 min read</span></p></div></div><article class="popover-hint"><p>hatena</p>
<pre><code>&lt;body>
*1392179496*Paper presentation: &quot;Variable-length n-gram language model based on hierarchical Pitman-Yor process&quot;
The paper says that existing methods cannot realistically be used for 8-gram language models, but this method is variable-length, so it is possible to make long grams only where necessary.

Create a tree with a Chinese restaurant process equivalent to the Pitman-Yor process: if sing comes after she will, add a guest corresponding to sing from the root node, following will and then she. Add proxy customers to the upper nodes as well.

With the existing method (HPYLM), the height to add a guest is fixed. However, instead of adding guests at a fixed height, we consider the &quot;probability that a guest will advance from the route and pass through the table&quot; for each node. This probability qi is unknown, how to estimate it?

Assume that the prior distribution of qi is a beta distribution, and place it first. Remove random words wt from it, update the parameters of the beta distribution from the distribution of the remaining customers, and add customers to a new depth by sampling from that beta distribution. This &quot;take and put&quot; process is repeated to converge to the correct distribution (Gibbs sampling).

There are two ways to do this: either the maximum depth is determined at the time of sampling, or it is done until the probability is sufficiently small. In the latter case, the parameter corresponding to n in the n-gram completely disappears, so we can call it an âˆ-gram.

The implementation uses a spray tree. Each node has a size of 64 bytes (assuming both int and ptr are 64-bit). 10000K nodes are enough to make a max 8-gram language model, i.e., about 640MB. Realistic size. The existing method requires storage space on the order of the 8th power of 30000 vocabulary words, which is not realistic, but the proposed method uses a long number of grams only where necessary, so it is feasible. In fact, the number of 5-gram contexts is about half of the number of 3-gram contexts, so the existing method, which requires 900 million times more memory than 3-grams for 5-grams, is not reasonable.

The perplexity of 3-gram and 5-gram corpus, which are possible with existing methods, is slightly higher than that of existing methods, but for English corpus, 7-gram and 8-gram corpus are lower than that of 5-gram corpus. The Japanese corpus...is not low. The âˆ-gram is also not low. This is a little disappointing, but at any rate, the reduction in the number of nodes is significant! This is the advantage of this method.

Thoughts: Google, please give me a language model made with max 8-grams!

PS: Added a note about what I did not understand when I tried to implement it. I found a field in the node definition that stores &quot;thw is the number of times w is estimated to have been generated from the parent node p(w|h'), not from p(w|h)&quot;. Also, only th is stored, but how is thw restored? d and Î¸ are estimated using the sampling formulas for dm and Î¸m that are included as an extra at the end of the original HPYLM paper, even though it didn't say how to estimate them?

** Implementation

https://github.com/nishio/VPYLM/blob/master/t.py
Note: I am ignoring the probability that a new table is created, so it does not work correctly yet. You can run it through, just to get a feel for it.

** Source.
http://chasen.org/~daiti-m/paper/ipsj07vpylm.pdf

next
** Finally
This article is written by the author, who is interested in Deep Learning, while reading and studying related papers. Therefore, it may contain mistakes. If you have any questions or comments, we would appreciate it if you could point them out to us.

*1392213182*Paper Introduction &quot;Continuous Space Topic Model Based on Gaussian Processes&quot;
The paper says that if you project a word in Deep Learning into a continuous Euclidean space, it is exciting that it expresses meaning, but that you can think of it as a normal distribution and optimize it instead of using a neural net, which is difficult to optimize to do that.

There is no direct reference to word2vec in the paper. Restricted Boltzmann Machines (RBMs) are generally said to surpass Letent Dirichlet Allocation (LDA), and it has been reported that &quot;Deep Learning&quot; with more layers will further improve performance, but optimization is difficult, it is hard to understand what is being learned, and it is difficult to connect to other models. It is difficult to optimize, it is difficult to know what it is learning, and it is difficult to connect to other models. That's why it is only used in research.

In RBM, the weights to the hidden layers encode the meaning, and in LDA, p(k|w) encodes the meaning, and ultimately the concept that &quot;a word corresponds to a d-dimensional continuous vector&quot; is important. So let us assume that there is an unknown function Ï† that maps a word to a d-dimensional vector, and that this d-dimensional vector Ï†(w) is Ï†(w) ~ N(0, I_d).

The probability of occurrence of a word is the default probability (which is obtained by maximum likelihood estimation) multiplied by exp(f(w)). f is a surface that follows a Gaussian process. The meaning of the Gaussian process is not yet well understood. In short, I imagine it is like a normal distribution with no fixed dimension.

Since f is difficult to compute, we introduce u, which follows a d-dimensional normal distribution, and set f = Î¦u. u corresponds one-to-one to the document. In short, documents are given a position in the same space as words, and information about which words are more likely to appear is expressed as an inner product.

The actual word frequency distribution sometimes looks like a Gaussian distribution, but the hem of the distribution is a little on the high frequency side, so we will use the Polya distribution with a0 as a parameter.

We want to get a good idea of the latent variables Ï†(w), u, and a0 from the observed word frequencies. Therefore, we use Metropolis Hasting (MH), one of the Markov Chain Monte Carlo (MCMC) methods. In short, we stop all but one variable, Markovianly determine the chosen variable from the original value, and if the new value is accepted by the MH criterion, we use it to update the value, and if not, we return to the original value.

Summary: The proposed CSTM can be regarded as a continuous Gaussian distribution of RBM, and can be easily optimized with MH. The problem occurs when the number of dimensions is too large due to the fact that it is a product model and not a mixture model, so the number of dimensions itself should be done in a Bayesian manner (like CRP?).

Thoughts: I understand that this time, since the model is that there is a u for each document, we can't directly do the same thing as word2vec. I wonder if it would be better to cut the model into windows of &quot;how many words before and after each word&quot; and consider each window to be a document. I haven't read the word2vec paper thoroughly yet, so I'll think about it after reading it. I also noticed that Dr. Masataka Goto is a co-author of the paper, and in the acknowledgements it says OngaCREST project, so I guess they are looking at not only natural language but also music applications. That sounds interesting.

** Source.
Paper: http://chasen.org/~daiti-m/paper/nl213cstm.pdf
Explanation of Gaussian processes but I have no idea: http://www.ism.ac.jp/~daichi/paper/svm2009advgp.pdf
MCMC and MH Explanation: http://ebsa.ism.ac.jp/ebooks/sp/sites/default/files/ebook/1881/pdf/vol3_ch10.pdf

** Finally
This article is written by the author, who is interested in Deep Learning, while reading and studying related papers. Therefore, it may contain mistakes. If you have any questions or comments, we would appreciate it if you could point them out to us.
&lt;/body>
</code></pre>
<h2 id="hatena-diary-2014-02-12"><a href="https://nishiohirokazu.hatenadiary.org/archive/2014/02/12" class="external">Hatena Diary 2014-02-12<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#hatena-diary-2014-02-12" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>This page is auto-translated from <a href="https://scrapbox.io/nishio/Hatena2014-02-12" class="external">/nishio/Hatena2014-02-12<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at <a href="https://twitter.com/nishio_en" class="external">@nishio_en<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. Iâ€™m very happy to spread my thought to non-Japanese readers.</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.4.0</a> Â© 2024</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="./postscript.js" type="module"></script></html>