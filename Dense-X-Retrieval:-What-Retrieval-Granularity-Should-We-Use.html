<!DOCTYPE html>
<html lang="en"><head><title>Dense X Retrieval: What Retrieval Granularity Should We Use?</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="ü™¥ Quartz 4.0"/><meta property="og:title" content="Dense X Retrieval: What Retrieval Granularity Should We Use?"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Dense X Retrieval: What Retrieval Granularity Should We Use?"/><meta name="twitter:description" content="With respect to granularity of RAG, the approach is to extract ‚Äúpropositions‚Äù that can be understood without context by replacing directive words, etc., to create an embedding of ..."/><meta property="og:description" content="With respect to granularity of RAG, the approach is to extract ‚Äúpropositions‚Äù that can be understood without context by replacing directive words, etc., to create an embedding of ..."/><meta property="og:image:type" content="image/webp"/><meta property="og:image:alt" content="With respect to granularity of RAG, the approach is to extract ‚Äúpropositions‚Äù that can be understood without context by replacing directive words, etc., to create an embedding of ..."/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:width" content="1200"/><meta property="og:height" content="630"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https:/quartz.jzhao.xyz/Dense-X-Retrieval:-What-Retrieval-Granularity-Should-We-Use"/><meta property="twitter:url" content="https:/quartz.jzhao.xyz/Dense-X-Retrieval:-What-Retrieval-Granularity-Should-We-Use"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="With respect to granularity of RAG, the approach is to extract ‚Äúpropositions‚Äù that can be understood without context by replacing directive words, etc., to create an embedding of ..."/><meta name="generator" content="Quartz"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("./static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Dense-X-Retrieval:-What-Retrieval-Granularity-Should-We-Use"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href=".">ü™¥ Quartz 4.0</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="./">Home</a><p> ‚ùØ </p></div><div class="breadcrumb-element"><a href>Dense X Retrieval: What Retrieval Granularity Should We Use?</a></div></nav><h1 class="article-title">Dense X Retrieval: What Retrieval Granularity Should We Use?</h1><p show-comma="true" class="content-meta"><span>May 01, 2024</span><span>5 min read</span></p></div></div><article class="popover-hint"><p>With respect to <a href="./granularity-of-RAG" class="internal alias" data-slug="granularity-of-RAG">granularity of RAG</a>, the approach is to extract ‚Äúpropositions‚Äù that can be understood without context by replacing directive words, etc., to create an embedding of propositional units.</p>
<blockquote>
<p>Dense X Retrieval: What Retrieval Granularity Should We Use?
Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, Dong Yu
Dense retrieval has become a prominent method to obtain relevant context or world knowledge in open-domain NLP tasks. When we use a learned dense retriever on a retrieval corpus at inference time, an often-overlooked design choice is the retrieval unit in which the corpus is indexed, e.g. document, passage, or sentence. We discover that the retrieval unit choice significantly impacts the performance of both retrieval and downstream tasks. Distinct from the typical approach of using passages or sentences, we introduce a novel retrieval unit, proposition, for dense retrieval. Propositions are defined as atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format. We conduct an empirical comparison of different retrieval granularity. Our results reveal that proposition-based retrieval significantly outperforms traditional passage or sentence-based methods in dense retrieval. Moreover, retrieval by proposition also enhances the performance of downstream QA tasks, since the retrieved texts are more condensed with question-relevant information, reducing the need for lengthy input tokens and minimizing the inclusion of extraneous, irrelevant information.
<a href="https://arxiv.org/abs/2312.06648" class="external">https://arxiv.org/abs/2312.06648<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></p>
</blockquote>
<img src="https://scrapbox.io/api/pages/nishio/claude/icon" alt="claude.icon" height="19.5"/>
- In document retrieval, it is clear that the granularity of the retrieval unit (document, sentence, proposition, etc.) has a significant impact on retrieval performance.
- Instead of the conventional document or sentence unit, a proposition is proposed as a new search unit. A proposition is the smallest unit of expression that represents a fact in a sentence.
- FACTOIDWIKI, which indexes Wikipedia at three granularities: 100-word documents, sentences, and propositions.
- Six different retrieval models were evaluated on five QA datasets, with propositional unit retrieval performing better than document or sentence unit retrieval.
- In particular, propositional unit retrieval was greatly superior in domains not included in the training data of the retrieval model.
- In the QA task, propositional unit search achieved the highest accuracy under the same token count restriction.
- Propositions are rich in contextual information, albeit in a compressed form, and thus are expected to have a higher density of question-related information.
- We hope that the findings of this research and FACTOIDWIKI will contribute to the development of information retrieval.
<p><img src="https://gyazo.com/ee2dd5043850ffe30038c6eb0f9d86e9/thumb/1000" alt="image"/>
<a href="./Proposition-indexing" class="internal alias" data-slug="Proposition-indexing">Proposition indexing</a></p>
<p><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>How do you index a proposition?</p>
<p><img src="https://scrapbox.io/api/pages/nishio/claude/icon" alt="claude.icon" height="19.5"/>The specific steps to index a proposition are as follows</p>
<ul>
<li>Divide each Wikipedia page into paragraphs.</li>
<li>To split each paragraph into propositions, the GPT-4 is used to create a dataset for propositional splitting.
<ul>
<li>Prompt GPT-4 to convert a paragraph into a list of propositions.</li>
<li>Collection of over 40,000 pairs of paragraph and propositional listings.</li>
</ul>
</li>
<li>The collected dataset is used to train a propositional partitioning model (Propositionizer).
<ul>
<li>Flan-T5-Large is used as the base model.</li>
<li>Fine-tuned to enter a paragraph and output a list of propositions.</li>
</ul>
</li>
<li>Split all Wikipedia pages into propositions using the learned Propositionizer.
<ul>
<li>Convert each paragraph into a list of propositions.</li>
<li>Ultimately, more than 250 million propositions are generated.</li>
</ul>
</li>
<li>Index the divided propositions.</li>
<li>Use <a href="./pyserini" class="internal" data-slug="pyserini">pyserini</a> and <a href="./faiss" class="internal" data-slug="faiss">faiss</a> libraries.</li>
<li>Each proposition is converted to an embedded vector in a batch process.</li>
<li>Vectors are indexed to enable efficient approximate nearest neighbor search.</li>
<li>When searching, the query is converted into an embedded vector, and the index is searched to efficiently retrieve relevant propositions.</li>
</ul>
<p>With the above procedure, FACTOIDWIKI can be constructed, indexing all pages of Wikipedia on a proposition-by-proposition basis. For indexing large propositions, parallelization of GPUs and distributed indexing are also utilized to accelerate online searches.</p>
<p>So you create teacher data with GPT4, train a model on that data, and then use it to process all the pages of Wikipedia?<img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/></p>
<p><img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>‚ÄúConvert a paragraph to a list of propositions by prompting GPT-4‚Äù more here</p>
<p><img src="https://scrapbox.io/api/pages/nishio/claude/icon" alt="claude.icon" height="19.5"/>The procedure for converting a paragraph into a list of propositions using GPT-4 is as follows</p>
<ul>
<li>Create a prompt that clearly defines the proposition and how it is to be split. For example, use the following prompt.
<ul>
<li>Use the following procedure to divide the given paragraph into a clear and simple list of propositions.</li>
<li>Split compound sentences into single sentences. Maintain the original expression as much as possible.</li>
<li>If there is additional information in the noun phrase, it is split into independent propositions.</li>
<li>Decontextualize] the proposition, e.g., by replacing the pronoun with the noun to which it refers.
<ul>
<li>I see.<img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/>
<ul>
<li>It would be easy to take the facts <a href="./out-of-context" class="internal alias" data-slug="out-of-context">out of context</a> and make them <a href="./atomic" class="internal" data-slug="atomic">atomic</a>, but is that generally the case?
<ul>
<li>Some activities, like <a href="./Fireworks-to-think-about" class="internal alias" data-slug="Fireworks-to-think-about">Fireworks to think about</a>, are a mesh of what was originally separated</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Output the results as a list in JSON format.</li>
</ul>
</li>
<li>After the prompt, give as input the Wikipedia paragraph you want to split into propositions.</li>
<li>Give GPT-4 a prompt and an input paragraph and have it generate a list of propositions.</li>
<li>Review the list of propositions generated and make manual corrections as necessary.</li>
<li>Repeat steps 2-4 until a proposition list of sufficient quality is obtained.</li>
<li>The following is an actual prompt and example output.</li>
</ul>
<p><img src="https://gyazo.com/3a0ab713a473485e7afbba4beecdb5a6/thumb/1000" alt="image"/></p>
<p>This procedure was performed on more than 40,000 paragraphs to create a dataset consisting of pairs of paragraphs and propositional lists. This large data set enabled us to effectively train the <a href="./propositional-partitioning-model" class="internal alias" data-slug="propositional-partitioning-model">propositional partitioning model</a>.</p>
<hr/>
<p>This page is auto-translated from [/nishio/Dense X Retrieval: What Retrieval Granularity Should We Use?](<a href="https://scrapbox.io/nishio/Dense" class="external">https://scrapbox.io/nishio/Dense<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> X Retrieval: What Retrieval Granularity Should We Use?) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at <a href="https://twitter.com/nishio_en" class="external">@nishio_en<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. I‚Äôm very happy to spread my thought to non-Japanese readers.</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.4.0</a> ¬© 2024</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="./postscript.js" type="module"></script></html>