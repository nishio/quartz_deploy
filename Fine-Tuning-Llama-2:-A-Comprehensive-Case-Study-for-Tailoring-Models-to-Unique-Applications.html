<!DOCTYPE html>
<html lang="en"><head><title>Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="ü™¥ Quartz 4.0"/><meta property="og:title" content="Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications"/><meta name="twitter:description" content="Fine tuning guide for [Llama2 Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications Fine tuning case studies by companies that provide fine ..."/><meta property="og:description" content="Fine tuning guide for [Llama2 Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications Fine tuning case studies by companies that provide fine ..."/><meta property="og:image:type" content="image/webp"/><meta property="og:image:alt" content="Fine tuning guide for [Llama2 Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications Fine tuning case studies by companies that provide fine ..."/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:width" content="1200"/><meta property="og:height" content="630"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https:/quartz.jzhao.xyz/Fine-Tuning-Llama-2:-A-Comprehensive-Case-Study-for-Tailoring-Models-to-Unique-Applications"/><meta property="twitter:url" content="https:/quartz.jzhao.xyz/Fine-Tuning-Llama-2:-A-Comprehensive-Case-Study-for-Tailoring-Models-to-Unique-Applications"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="Fine tuning guide for [Llama2 Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications Fine tuning case studies by companies that provide fine ..."/><meta name="generator" content="Quartz"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("./static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Fine-Tuning-Llama-2:-A-Comprehensive-Case-Study-for-Tailoring-Models-to-Unique-Applications"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href=".">ü™¥ Quartz 4.0</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="./">Home</a><p> ‚ùØ </p></div><div class="breadcrumb-element"><a href>Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications</a></div></nav><h1 class="article-title">Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications</h1><p show-comma="true" class="content-meta"><span>Aug 14, 2023</span><span>4 min read</span></p></div></div><article class="popover-hint"><p>Fine tuning guide for [Llama2</p>
<ul>
<li><a href="https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications" class="external">Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
<p>Fine tuning case studies by companies that provide fine tuning services using their own services</p>
<ul>
<li>We should think of it as, ‚ÄúHere‚Äôs an example of what worked best out of the many we tried.‚Äù</li>
</ul>
<p>Llama-2 model</p>
<ul>
<li>Validated with three real-world use cases</li>
<li>Fine tuning improves accuracy</li>
</ul>
<p>Better than GPT-4 in some niche cases</p>
<ul>
<li>
<p>Functional Expression Extraction from Unstructured Text (ViGGO)</p>
</li>
<li>
<p>SQL generation (SQL-create-context)</p>
</li>
<li>
<p>7B is sufficient for both.</p>
<ul>
<li>On the other hand, as for the math comprehension task GSM, even 70B doesn‚Äôt quite catch up.</li>
</ul>
</li>
<li>
<p>In particular, Llama-13b improved accuracy from 58% to 98% in function representation, from 42% to 89% in SQL generation, and from 28% to 47% in GSM.</p>
</li>
</ul>
<p>Fine-tuning basics
In all three tasks, we use standard fine tuning techniques for all parameters.</p>
<ul>
<li>The model is fine-tuned to predict the next token</li>
<li>All parameters in the model are subject to gradient update</li>
<li>Block freezing and LoRA are not used.</li>
<li>Ray makes it easy (advertisement).</li>
</ul>
<p>Shard data between workers
Model sharding with DeepSpeed</p>
<p>special token</p>
<ul>
<li>Structuring tasks with special tokens instead of directing them with natural sentences
<ul>
<li>Does that learning improve performance for instructions in natural text? Does it ignore how to convert them otherwise?</li>
<li>Sounds like the latter, that if for some reason structured input can be obtained, it is better to use tokens that do not appear in natural text to clearly convey structure.</li>
</ul>
</li>
</ul>
<p>Explanation of ViGGO</p>
<ul>
<li>Okay, it looks like the problem is that we expect to interact according to fairly strict rules, and if we communicate those strict rules in natural sentences, even GPT4 can‚Äôt do it well.</li>
</ul>
<p>Effectiveness of Fine Tuning</p>
<blockquote>
<p>In an earlier blog post, we discussed the idea that fine tuning is not about facts, but about form.</p>
</blockquote>
<ul>
<li><a href="https://www.anyscale.com/blog/fine-tuning-is-for-form-not-facts" class="external">Fine tuning is for form, not facts | Anyscale<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a></li>
</ul>
<p>Some important questions</p>
<ul>
<li>Does the base model encounter the task concept in the learning process?
<ul>
<li>New concepts not encountered are unlikely to be acquired through small-scale fine tuning.</li>
</ul>
</li>
<li>Will Fewshots improve the situation?
<ul>
<li>If it improves, fine tuning is likely to improve it further.</li>
<li>
<blockquote>
<p>because far more examples can be incorporated into the weights of the neural network inside the model.</p>
</blockquote>
</li>
</ul>
</li>
</ul>
<p>ViGGO revolves around pattern recognition and requires a basic grasp of language and basic concepts, but does not require complex logical reasoning.</p>
<ul>
<li>More importantly: all the ‚Äúfacts‚Äù needed for the output are already embedded in the input</li>
<li>I see, that type of task is why ViGGO was able to exceed GPT4 with 7B fine tuning.<img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/></li>
</ul>
<p>evaluation</p>
<ul>
<li>GPT4 is not very good at ‚Äúkeep the attributes in order‚Äù and by putting that on the determination of whether it‚Äôs successful or not, it‚Äôs down from 90% to 50%.</li>
<li>Fine tuning keeps order.</li>
</ul>
<p>SQL generation with Llama-2 fine-tuning model</p>
<ul>
<li>Why is fine-tuning promising?
<ul>
<li>
<blockquote>
<p>The success of this task depends on the LLM‚Äôs ability to learn the ‚Äústructure‚Äù of SQL and translate natural language into this structure</p>
</blockquote>
</li>
<li>I guess this also means that the ‚Äúform of output‚Äù is another pattern that‚Äôs important to follow the rules tightly.<img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/></li>
</ul>
</li>
</ul>
<p>result</p>
<ul>
<li>7B fine tuning outperformed GPT-4 and 70B-chat
<ul>
<li>Seems to me that being learned for chatting would return and not follow the format.<img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/></li>
</ul>
</li>
</ul>
<p>Arithmetic Reasoning for Elementary School Students (GSM8k)</p>
<ul>
<li>
<blockquote>
<p>The fine-tuning task on this data set is different from the previous two. As opposed to simply learning the structure, we wanted to see how well LLM could improve our ability to reason about math problems.</p>
</blockquote>
</li>
<li>
<p>Cut out at GPT-3.5 because it is difficult to verify if the answer is correctly answered when the answer is given in natural sentences.</p>
<ul>
<li>Fine tuning the output so that it can be immediately cut out with a regular expression has reduced the cost of API calls.</li>
</ul>
</li>
</ul>
<p>The chat version has higher performance in 7B and 13B to begin with.</p>
<ul>
<li>Maybe the training data contains mathematical interactions.</li>
<li>The chat version of the 70B 8-shot baseline is losing all of its</li>
<li>Fine tuning increases performance and lowers token costs.</li>
</ul>
<p>They‚Äôve decided that 8k data points isn‚Äôt enough, so they‚Äôve taken the approach of increasing it even more, and they‚Äôre saying it‚Äôs even better.</p>
<ul>
<li>You‚Äôre doing a great job.<img src="https://scrapbox.io/api/pages/nishio/nishio/icon" alt="nishio.icon" height="19.5"/></li>
</ul>
<hr/>
<p>This page is auto-translated from [/nishio/Fine-Tuning Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications](<a href="https://scrapbox.io/nishio/Fine-Tuning" class="external">https://scrapbox.io/nishio/Fine-Tuning<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> Llama-2: A Comprehensive Case Study for Tailoring Models to Unique Applications) using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at <a href="https://twitter.com/nishio_en" class="external">@nishio_en<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. I‚Äôm very happy to spread my thought to non-Japanese readers.</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li><a href="./Diary-2023-08-13" class="internal">Diary 2023-08-13</a></li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.4.0</a> ¬© 2024</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="./postscript.js" type="module"></script></html>