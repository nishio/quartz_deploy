<!DOCTYPE html>
<html lang="en"><head><title>Hatena2014-02-08</title><meta charset="utf-8"/><link rel="preconnect" href="https://fonts.googleapis.com"/><link rel="preconnect" href="https://fonts.gstatic.com"/><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=IBM Plex Mono&amp;family=Schibsted Grotesk:wght@400;700&amp;family=Source Sans Pro:ital,wght@0,400;0,600;1,400;1,600&amp;display=swap"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><meta name="og:site_name" content="ğŸª´ Quartz 4.0"/><meta property="og:title" content="Hatena2014-02-08"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Hatena2014-02-08"/><meta name="twitter:description" content="hatena &lt;body> *1391792338*Deep Learning Paper Introduction &quot;Generating Text with Recurrent Neural Networks&quot; A story about using recurrent neural networks ..."/><meta property="og:description" content="hatena &lt;body> *1391792338*Deep Learning Paper Introduction &quot;Generating Text with Recurrent Neural Networks&quot; A story about using recurrent neural networks ..."/><meta property="og:image:type" content="image/webp"/><meta property="og:image:alt" content="hatena &lt;body> *1391792338*Deep Learning Paper Introduction &quot;Generating Text with Recurrent Neural Networks&quot; A story about using recurrent neural networks ..."/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:width" content="1200"/><meta property="og:height" content="630"/><meta property="og:image:url" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta name="twitter:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="og:image" content="https://quartz.jzhao.xyz/static/og-image.png"/><meta property="twitter:domain" content="quartz.jzhao.xyz"/><meta property="og:url" content="https:/quartz.jzhao.xyz/Hatena2014-02-08"/><meta property="twitter:url" content="https:/quartz.jzhao.xyz/Hatena2014-02-08"/><link rel="icon" href="./static/icon.png"/><meta name="description" content="hatena &lt;body> *1391792338*Deep Learning Paper Introduction &quot;Generating Text with Recurrent Neural Networks&quot; A story about using recurrent neural networks ..."/><meta name="generator" content="Quartz"/><link href="./index.css" rel="stylesheet" type="text/css" spa-preserve/><link href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" rel="stylesheet" type="text/css" spa-preserve/><script src="./prescript.js" type="application/javascript" spa-preserve></script><script type="application/javascript" spa-preserve>const fetchData = fetch("./static/contentIndex.json").then(data => data.json())</script></head><body data-slug="Hatena2014-02-08"><div id="quartz-root" class="page"><div id="quartz-body"><div class="left sidebar"><h2 class="page-title"><a href=".">ğŸª´ Quartz 4.0</a></h2><div class="spacer mobile-only"></div><div class="search"><button class="search-button" id="search-button"><p>Search</p><svg role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title>Search</title><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"></path><circle cx="8" cy="8" r="7"></circle></g></svg></button><div id="search-container"><div id="search-space"><input autocomplete="off" id="search-bar" name="search" type="text" aria-label="Search for something" placeholder="Search for something"/><div id="search-layout" data-preview="true"></div></div></div></div><button class="darkmode" id="darkmode"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="dayIcon" x="0px" y="0px" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35" xml:space="preserve" aria-label="Dark mode"><title>Dark mode</title><path d="M6,17.5C6,16.672,5.328,16,4.5,16h-3C0.672,16,0,16.672,0,17.5    S0.672,19,1.5,19h3C5.328,19,6,18.328,6,17.5z M7.5,26c-0.414,0-0.789,0.168-1.061,0.439l-2,2C4.168,28.711,4,29.086,4,29.5    C4,30.328,4.671,31,5.5,31c0.414,0,0.789-0.168,1.06-0.44l2-2C8.832,28.289,9,27.914,9,27.5C9,26.672,8.329,26,7.5,26z M17.5,6    C18.329,6,19,5.328,19,4.5v-3C19,0.672,18.329,0,17.5,0S16,0.672,16,1.5v3C16,5.328,16.671,6,17.5,6z M27.5,9    c0.414,0,0.789-0.168,1.06-0.439l2-2C30.832,6.289,31,5.914,31,5.5C31,4.672,30.329,4,29.5,4c-0.414,0-0.789,0.168-1.061,0.44    l-2,2C26.168,6.711,26,7.086,26,7.5C26,8.328,26.671,9,27.5,9z M6.439,8.561C6.711,8.832,7.086,9,7.5,9C8.328,9,9,8.328,9,7.5    c0-0.414-0.168-0.789-0.439-1.061l-2-2C6.289,4.168,5.914,4,5.5,4C4.672,4,4,4.672,4,5.5c0,0.414,0.168,0.789,0.439,1.06    L6.439,8.561z M33.5,16h-3c-0.828,0-1.5,0.672-1.5,1.5s0.672,1.5,1.5,1.5h3c0.828,0,1.5-0.672,1.5-1.5S34.328,16,33.5,16z     M28.561,26.439C28.289,26.168,27.914,26,27.5,26c-0.828,0-1.5,0.672-1.5,1.5c0,0.414,0.168,0.789,0.439,1.06l2,2    C28.711,30.832,29.086,31,29.5,31c0.828,0,1.5-0.672,1.5-1.5c0-0.414-0.168-0.789-0.439-1.061L28.561,26.439z M17.5,29    c-0.829,0-1.5,0.672-1.5,1.5v3c0,0.828,0.671,1.5,1.5,1.5s1.5-0.672,1.5-1.5v-3C19,29.672,18.329,29,17.5,29z M17.5,7    C11.71,7,7,11.71,7,17.5S11.71,28,17.5,28S28,23.29,28,17.5S23.29,7,17.5,7z M17.5,25c-4.136,0-7.5-3.364-7.5-7.5    c0-4.136,3.364-7.5,7.5-7.5c4.136,0,7.5,3.364,7.5,7.5C25,21.636,21.636,25,17.5,25z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" id="nightIcon" x="0px" y="0px" viewBox="0 0 100 100" style="enable-background:new 0 0 100 100" xml:space="preserve" aria-label="Light mode"><title>Light mode</title><path d="M96.76,66.458c-0.853-0.852-2.15-1.064-3.23-0.534c-6.063,2.991-12.858,4.571-19.655,4.571  C62.022,70.495,50.88,65.88,42.5,57.5C29.043,44.043,25.658,23.536,34.076,6.47c0.532-1.08,0.318-2.379-0.534-3.23  c-0.851-0.852-2.15-1.064-3.23-0.534c-4.918,2.427-9.375,5.619-13.246,9.491c-9.447,9.447-14.65,22.008-14.65,35.369  c0,13.36,5.203,25.921,14.65,35.368s22.008,14.65,35.368,14.65c13.361,0,25.921-5.203,35.369-14.65  c3.872-3.871,7.064-8.328,9.491-13.246C97.826,68.608,97.611,67.309,96.76,66.458z"></path></svg></button></div><div class="center"><div class="page-header"><div class="popover-hint"><nav class="breadcrumb-container" aria-label="breadcrumbs"><div class="breadcrumb-element"><a href="./">Home</a><p> â¯ </p></div><div class="breadcrumb-element"><a href>Hatena2014-02-08</a></div></nav><h1 class="article-title">Hatena2014-02-08</h1><p show-comma="true" class="content-meta"><span>Feb 08, 2014</span><span>7 min read</span></p></div></div><article class="popover-hint"><p>hatena</p>
<pre><code>&lt;body>
*1391792338*Deep Learning Paper Introduction &quot;Generating Text with Recurrent Neural Networks&quot;
A story about using recurrent neural networks (RNNs) to generate sentences.

RNN is powerful but hard to learn; Hessian-free optimization (HF) can be used for nice and difficult problems.

In this paper, a language model is created using letters as input, from which sentences are generated.

I had some problems with the standard RNN, so I made a multiplicative variant. The transition matrix of the vector from the hidden layer to the next layer can be selected according to the input character.

The multiplicative RNN (MRNN) was trained on HF on a machine with 8 high-end GPUs over 5 days and outperformed existing language models.

The activation function of the hidden layer is tanh and the output layer is bare.

Sequence memoizer can handle a data set of about 130 MB on a machine with 32 GB memory due to its data structure, while MRNN does not have this upper limit.

The experiment used 100 MB of data consisting of 86 different characters. There are 1,500 hidden layers and 1,500 FACTORS, for a total of 4.9 million parameters. The weights were initialized to sparse, which corresponds to 500 hidden layers when BPTT is unrolled.

(Incidentally, the scanned data of one copy of Iwanami Bunko I have on hand is 530KB, so 100MB data is roughly equivalent to 200 copies.)

Experimental results: Bits per character are smaller than those of memorizer. When a sentence is generated, words with a clear meaning are generated.

Impressions Having played with the n-gram model to generate sentences, I'd like to try this to see how much better it is. It's interesting to see that words are created even though the text is used as input, but I can't do this alone to determine &quot;what range is a word&quot;. I'll see what I can do.

** Source.
http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Sutskever_524.pdf

** Finally
This article is written by the author, who is interested in Deep Learning, while reading and studying related papers. Therefore, it may contain mistakes. If you have any questions or comments, we would appreciate it if you could point them out to us.

*1391825340*Deep Learning Paper Introduction &quot;Learning Recurrent Neural Networks with Hessian-Free Optimization&quot;
The story is that it was a difficult problem to train a recurrent neural network (RNN) to learn long-range correlations, but using Hessian-Free, it was possible.

The advantage of RNN is that it can be easily computed with Back Propagation Through Time (BPTT) + stochastic gradient method, but correlations as far apart as 10 time steps cannot be learned at all with a first-order gradient method

The cause is &quot;vanishing/exploding gradients. Long-range correlations are propagated back to the previous time layer many times in BPTT, so the error signal quickly decays and disappears.

The recently developed Hessian-Free (HF), also known as truncated-Newton or Newton-CG, is effective for training Deep Neural Networks, so it must also be effective for training RNNs.

I can't summarize the formulas too succinctly, so I'll skip them. If you don't read this section carefully, this paper won't make any sense.

To put it briefly, it seems to be that &quot;the method of approximating the objective function using the second-order derivative and optimizing it is very computationally expensive because the inverse matrix of the Hessian (the matrix of the second-order derivative) needs to be computed,&quot; but I did not understand how Hessian-Free avoids this problem.

Hessian-Free&quot; itself is explained in &quot;Deep learning via Hessian-free optimization&quot; by the same author, James Martens. Should I read this first?

** Source.
http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf

** Finally
This article is written by the author, who is interested in Deep Learning, while reading and studying related papers. Therefore, it may contain mistakes. If you have any questions or comments, we would appreciate it if you could point them out to us.

*1391838220*Deep Learning Paper Introduction &quot;Deep learning via Hessian-free optimization&quot;
The new optimization method called &quot;Hessian-Free&quot; was used to train an auto-encoder for Deep Learning, and it exceeded the performance of the existing report without prior training.

Parameter determination for neural nets is a well-studied problem and is said to be efficiently computed by the gradient method. However, it does not work well in cases with a very large number of hidden layers, such as in Deep Learning. It can take a long time to learn or even perform badly on the training data (under-fitting).

It is well known among optimization researchers that gradient methods are unstable for objective functions with pathological curvature; second-order optimization methods work well for such objective functions. So it would be a good idea to use this kind of optimization for Deep Learning.

But there are still a few problems. First, we need to make it work at a realistic speed for large data sets. This could be done with online learning, like stochastic gradient descent (SGD).

Pre-training layer by layer is another big step forward. Doing this and then SGD appears to avoid problems. In fact, there are various successful applications. But the question remains: why does this work? Why is it necessary? Some researchers have explained that it is due to the large number of local minima.

Another explanation is that the objective function has pathological curvature (e.g., elongated valleys) and is not optimizable by optimization methods that do not look at curvature, such as the gradient method. Therefore, from this perspective, we propose a semi-online second-order optimization method. This method is not under-fitting and is more efficient than existing methods using pre-training.

First, we will discuss the standard second-order optimization method, the Newton method. The Newton method requires the computation of a Hessian the size of the square of the number of parameters, which is not realistic for computing large models. However, by learning this, one can understand how the more realistic variants (called quasi-Newton methods) behave.

The Newton method, like the gradient method, is a method in which parameters are updated sequentially. The core idea is to &quot;approximate the objective function f by a quadratic function&quot; (Equation 1) You can think of B as a Hessian, but since it is sometimes not positive definite and does not have a minimum value, you increase the diagonal components appropriately to make it positive definite (dump)

Scale invariance&quot; is an important feature of Newton's method. Linear transformations do not change the behavior. Without this feature, poor parameter scaling will result in poor performance. This feature also eliminates the need to adjust the learning rate.

Conversely, one can think of it as an implicit scaling based on the curvature around the current parameters, which transforms the gradient.

If the absolute value of curvature is small, it means that the change in gradient is small, so it is OK to go a long distance. On the other hand, if it is large, it means that you are sensitive to the direction of p, so you should go a little further and reconfirm the appropriate direction of p. (I will reconfirm the formula later in this paragraph. (I will recheck the formula later in this paragraph.)

(I haven't gotten to the important stuff yet, but I'll get to the rest of it later)

** Source.
http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_Martens10.pdf

** Finally
This article is written by the author, who is interested in Deep Learning, while reading and studying related papers. Therefore, it may contain mistakes. If you have any questions or comments, we would appreciate it if you could point them out to us.
 
&lt;/body>
</code></pre>
<h2 id="hatena-diary-2014-02-08"><a href="https://nishiohirokazu.hatenadiary.org/archive/2014/02/08" class="external">Hatena Diary 2014-02-08<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a><a role="anchor" aria-hidden="true" tabindex="-1" data-no-popover="true" href="#hatena-diary-2014-02-08" class="internal"><svg width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg></a></h2>
<p>This page is auto-translated from <a href="https://scrapbox.io/nishio/Hatena2014-02-08" class="external">/nishio/Hatena2014-02-08<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a> using DeepL. If you looks something interesting but the auto-translated English is not good enough to understand it, feel free to let me know at <a href="https://twitter.com/nishio_en" class="external">@nishio_en<svg aria-hidden="true" class="external-icon" style="max-width:0.8em;max-height:0.8em;" viewBox="0 0 512 512"><path d="M320 0H288V64h32 82.7L201.4 265.4 178.7 288 224 333.3l22.6-22.6L448 109.3V192v32h64V192 32 0H480 320zM32 32H0V64 480v32H32 456h32V480 352 320H424v32 96H64V96h96 32V32H160 32z"></path></svg></a>. Iâ€™m very happy to spread my thought to non-Japanese readers.</p></article><hr/><div class="page-footer"></div></div><div class="right sidebar"><div class="graph"><h3>Graph View</h3><div class="graph-outer"><div id="graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:1,&quot;scale&quot;:1.1,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:false}"></div><button id="global-graph-icon" aria-label="Global Graph"><svg version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 55 55" fill="currentColor" xml:space="preserve"><path d="M49,0c-3.309,0-6,2.691-6,6c0,1.035,0.263,2.009,0.726,2.86l-9.829,9.829C32.542,17.634,30.846,17,29,17
                s-3.542,0.634-4.898,1.688l-7.669-7.669C16.785,10.424,17,9.74,17,9c0-2.206-1.794-4-4-4S9,6.794,9,9s1.794,4,4,4
                c0.74,0,1.424-0.215,2.019-0.567l7.669,7.669C21.634,21.458,21,23.154,21,25s0.634,3.542,1.688,4.897L10.024,42.562
                C8.958,41.595,7.549,41,6,41c-3.309,0-6,2.691-6,6s2.691,6,6,6s6-2.691,6-6c0-1.035-0.263-2.009-0.726-2.86l12.829-12.829
                c1.106,0.86,2.44,1.436,3.898,1.619v10.16c-2.833,0.478-5,2.942-5,5.91c0,3.309,2.691,6,6,6s6-2.691,6-6c0-2.967-2.167-5.431-5-5.91
                v-10.16c1.458-0.183,2.792-0.759,3.898-1.619l7.669,7.669C41.215,39.576,41,40.26,41,41c0,2.206,1.794,4,4,4s4-1.794,4-4
                s-1.794-4-4-4c-0.74,0-1.424,0.215-2.019,0.567l-7.669-7.669C36.366,28.542,37,26.846,37,25s-0.634-3.542-1.688-4.897l9.665-9.665
                C46.042,11.405,47.451,12,49,12c3.309,0,6-2.691,6-6S52.309,0,49,0z M11,9c0-1.103,0.897-2,2-2s2,0.897,2,2s-0.897,2-2,2
                S11,10.103,11,9z M6,51c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S8.206,51,6,51z M33,49c0,2.206-1.794,4-4,4s-4-1.794-4-4
                s1.794-4,4-4S33,46.794,33,49z M29,31c-3.309,0-6-2.691-6-6s2.691-6,6-6s6,2.691,6,6S32.309,31,29,31z M47,41c0,1.103-0.897,2-2,2
                s-2-0.897-2-2s0.897-2,2-2S47,39.897,47,41z M49,10c-2.206,0-4-1.794-4-4s1.794-4,4-4s4,1.794,4,4S51.206,10,49,10z"></path></svg></button></div><div id="global-graph-outer"><div id="global-graph-container" data-cfg="{&quot;drag&quot;:true,&quot;zoom&quot;:true,&quot;depth&quot;:-1,&quot;scale&quot;:0.9,&quot;repelForce&quot;:0.5,&quot;centerForce&quot;:0.3,&quot;linkDistance&quot;:30,&quot;fontSize&quot;:0.6,&quot;opacityScale&quot;:1,&quot;showTags&quot;:true,&quot;removeTags&quot;:[],&quot;focusOnHover&quot;:true}"></div></div></div><div class="backlinks"><h3>Backlinks</h3><ul class="overflow"><li>No backlinks found</li></ul></div></div><footer class><p>Created with <a href="https://quartz.jzhao.xyz/">Quartz v4.4.0</a> Â© 2024</p><ul><li><a href="https://github.com/jackyzha0/quartz">GitHub</a></li><li><a href="https://discord.gg/cRFFHYye7t">Discord Community</a></li></ul></footer></div></div></body><script type="application/javascript">function c(){let t=this.parentElement;t.classList.toggle("is-collapsed");let l=t.classList.contains("is-collapsed")?this.scrollHeight:t.scrollHeight;t.style.maxHeight=l+"px";let o=t,e=t.parentElement;for(;e;){if(!e.classList.contains("callout"))return;let n=e.classList.contains("is-collapsed")?e.scrollHeight:e.scrollHeight+o.scrollHeight;e.style.maxHeight=n+"px",o=e,e=e.parentElement}}function i(){let t=document.getElementsByClassName("callout is-collapsible");for(let s of t){let l=s.firstElementChild;if(l){l.addEventListener("click",c),window.addCleanup(()=>l.removeEventListener("click",c));let e=s.classList.contains("is-collapsed")?l.scrollHeight:s.scrollHeight;s.style.maxHeight=e+"px"}}}document.addEventListener("nav",i);window.addEventListener("resize",i);
</script><script src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/copy-tex.min.js" type="application/javascript"></script><script src="./postscript.js" type="module"></script></html>